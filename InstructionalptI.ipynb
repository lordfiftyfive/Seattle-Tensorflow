{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InstructionalptI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOCOLrtArCGqx2AwSCuns1o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordfiftyfive/Seattle-Tensorflow/blob/master/InstructionalptI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91CKWv2HO24E",
        "colab_type": "text"
      },
      "source": [
        "If you do NOT have the following libraries installed on your local machine then you will want to:\n",
        " 1. install anaconda navigator and install the spyder enviroment withen anaconda navigator \n",
        " 2. open up a terminal withen anaconda navigator and use conda install (name of library)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFvCEJw9MlPl",
        "colab_type": "code",
        "outputId": "278bec05-ca3e-4a4a-8d60-b5f7cda13208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "!pip install quandl\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting quandl\n",
            "  Downloading https://files.pythonhosted.org/packages/07/ab/8cd479fba8a9b197a43a0d55dd534b066fb8e5a0a04b5c0384cbc5d663aa/Quandl-3.5.0-py2.py3-none-any.whl\n",
            "Collecting inflection>=0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/35/a6eb45b4e2356fe688b21570864d4aa0d0a880ce387defe9c589112077f8/inflection-0.3.1.tar.gz\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl) (8.0.2)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.25.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from quandl) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from quandl) (1.17.5)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from quandl) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from quandl) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2.8)\n",
            "Building wheels for collected packages: inflection\n",
            "  Building wheel for inflection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for inflection: filename=inflection-0.3.1-cp36-none-any.whl size=6076 sha256=5acd675758d0c135d4489884d8a94d4c7e34029acdab1782fbdd9439267dbe8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/5a/d3/6fc3bf6516d2a3eb7e18f9f28b472110b59325f3f258fe9211\n",
            "Successfully built inflection\n",
            "Installing collected packages: inflection, quandl\n",
            "Successfully installed inflection-0.3.1 quandl-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfNwAr7UNjHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import quandl\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYoIdS-gKi3v",
        "colab_type": "text"
      },
      "source": [
        "    First we are going to want to import our data here we are going to show how\n",
        "     to import using one of the most common data format which is the csv file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAAC80oFgrvn",
        "colab_type": "code",
        "outputId": "117c4255-762e-4be8-d3f0-925e27bab2d6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e2c77bc0-7fbd-4ac3-ad7e-48f0aa45c6aa\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e2c77bc0-7fbd-4ac3-ad7e-48f0aa45c6aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 930-data-export.csv to 930-data-export (1).csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'930-data-export.csv': b'\\xef\\xbb\\xbf\"Region Code\",\"Timestamp (Hour Ending)\",\"Demand (MWh)\",\"Demand Forecast (MWh)\",\"Net Generation (MWh)\",\"Total Interchange (MWh)\"\\nUS48,\"1/13/2020 12 a.m. EST\",412290,407010,392570,2977\\nUS48,\"1/13/2020 1 a.m. EST\",396187,389484,384466,3557\\nUS48,\"1/13/2020 2 a.m. EST\",383358,378106,373763,3755\\nUS48,\"1/13/2020 3 a.m. EST\",376657,371685,368427,3617\\nUS48,\"1/13/2020 4 a.m. EST\",373911,371433,365626,3968\\nUS48,\"1/13/2020 5 a.m. EST\",377435,379238,372388,3885\\nUS48,\"1/13/2020 6 a.m. EST\",391321,398288,390232,3724\\nUS48,\"1/13/2020 7 a.m. EST\",420939,426194,419251,4251\\nUS48,\"1/13/2020 8 a.m. EST\",451411,449840,445065,4899\\nUS48,\"1/13/2020 9 a.m. EST\",466590,462151,458984,3777\\nUS48,\"1/13/2020 10 a.m. EST\",474128,466698,463784,2587\\nUS48,\"1/13/2020 11 a.m. EST\",476935,467473,465528,2413\\nUS48,\"1/13/2020 12 p.m. EST\",473527,463802,464555,3258\\nUS48,\"1/13/2020 1 p.m. EST\",469042,458334,459451,3658\\nUS48,\"1/13/2020 2 p.m. EST\",463807,454026,454014,3540\\nUS48,\"1/13/2020 3 p.m. EST\",461050,450478,450718,2629\\nUS48,\"1/13/2020 4 p.m. EST\",482699,449834,449548,2667\\nUS48,\"1/13/2020 5 p.m. EST\",457355,456182,454486,3712\\nUS48,\"1/13/2020 6 p.m. EST\",466943,464312,461205,4701\\nUS48,\"1/13/2020 7 p.m. EST\",479553,473890,469744,4811\\nUS48,\"1/13/2020 8 p.m. EST\",483558,476427,473429,2882\\nUS48,\"1/13/2020 9 p.m. EST\",479106,472856,465850,4294\\nUS48,\"1/13/2020 10 p.m. EST\",465532,458270,449632,3168\\nUS48,\"1/13/2020 11 p.m. EST\",468760,437842,432086,2516\\nUS48,\"1/14/2020 12 a.m. EST\",426254,416012,405418,2353\\nUS48,\"1/14/2020 1 a.m. EST\",402196,397248,390650,2659\\nUS48,\"1/14/2020 2 a.m. EST\",393223,384741,380533,1929\\nUS48,\"1/14/2020 3 a.m. EST\",382796,376001,372151,2638\\nUS48,\"1/14/2020 4 a.m. EST\",378596,373408,368678,1984\\nUS48,\"1/14/2020 5 a.m. EST\",379293,379332,372224,2172\\nUS48,\"1/14/2020 6 a.m. EST\",389607,396075,384969,1834\\nUS48,\"1/14/2020 7 a.m. EST\",413958,421503,410263,2027\\nUS48,\"1/14/2020 8 a.m. EST\",439522,443869,431831,3126\\nUS48,\"1/14/2020 9 a.m. EST\",451986,454713,444514,3983\\nUS48,\"1/14/2020 10 a.m. EST\",459440,460033,449327,3426\\nUS48,\"1/14/2020 11 a.m. EST\",462488,461730,451464,3146\\nUS48,\"1/14/2020 12 p.m. EST\",460619,458973,450228,3225\\nUS48,\"1/14/2020 1 p.m. EST\",457269,455698,447608,3399\\nUS48,\"1/14/2020 2 p.m. EST\",454194,453132,444387,3722\\nUS48,\"1/14/2020 3 p.m. EST\",451344,451396,441137,2448\\nUS48,\"1/14/2020 4 p.m. EST\",475519,451950,441885,2692\\nUS48,\"1/14/2020 5 p.m. EST\",430021,457626,445378,2934\\nUS48,\"1/14/2020 6 p.m. EST\",460054,465145,452756,3774\\nUS48,\"1/14/2020 7 p.m. EST\",470792,473445,457782,3160\\nUS48,\"1/14/2020 8 p.m. EST\",474856,475349,459931,1584\\nUS48,\"1/14/2020 9 p.m. EST\",472403,471739,455844,1124\\nUS48,\"1/14/2020 10 p.m. EST\",460373,457830,442975,1406\\nUS48,\"1/14/2020 11 p.m. EST\",464057,437439,424745,506\\nUS48,\"1/15/2020 12 a.m. EST\",423781,415531,408584,1553\\nUS48,\"1/15/2020 1 a.m. EST\",404545,398414,391260,2385\\nUS48,\"1/15/2020 2 a.m. EST\",390897,383257,379225,3133\\nUS48,\"1/15/2020 3 a.m. EST\",381912,374153,370792,2438\\nUS48,\"1/15/2020 4 a.m. EST\",376392,371155,367256,2993\\nUS48,\"1/15/2020 5 a.m. EST\",377778,376498,370556,2357\\nUS48,\"1/15/2020 6 a.m. EST\",386077,393515,382691,1650\\nUS48,\"1/15/2020 7 a.m. EST\",411111,418421,405967,1977\\nUS48,\"1/15/2020 8 a.m. EST\",435987,440003,425928,1769\\nUS48,\"1/15/2020 9 a.m. EST\",447084,450958,436535,2195\\nUS48,\"1/15/2020 10 a.m. EST\",452808,456744,439517,1282\\nUS48,\"1/15/2020 11 a.m. EST\",453618,458837,440404,1511\\nUS48,\"1/15/2020 12 p.m. EST\",452786,456530,440169,1098\\nUS48,\"1/15/2020 1 p.m. EST\",449322,453996,437929,2121\\nUS48,\"1/15/2020 2 p.m. EST\",446829,451406,436209,2677\\nUS48,\"1/15/2020 3 p.m. EST\",445870,450391,436247,2975\\nUS48,\"1/15/2020 4 p.m. EST\",446390,451163,438899,3178\\nUS48,\"1/15/2020 5 p.m. EST\",450168,457255,445729,3362\\nUS48,\"1/15/2020 6 p.m. EST\",459824,464361,453110,3740\\nUS48,\"1/15/2020 7 p.m. EST\",469828,472660,457287,3146\\nUS48,\"1/15/2020 8 p.m. EST\",473300,474951,460937,3711\\nUS48,\"1/15/2020 9 p.m. EST\",469198,470495,454558,3225\\nUS48,\"1/15/2020 10 p.m. EST\",458489,456152,442481,3191\\nUS48,\"1/15/2020 11 p.m. EST\",441342,436164,424819,2232\\nUS48,\"1/16/2020 12 a.m. EST\",421109,414336,407062,1957\\nUS48,\"1/16/2020 1 a.m. EST\",402523,396276,387895,2227\\nUS48,\"1/16/2020 2 a.m. EST\",389979,383396,376707,2857\\nUS48,\"1/16/2020 3 a.m. EST\",381205,374526,368137,2818\\nUS48,\"1/16/2020 4 a.m. EST\",376492,372192,366204,3257\\nUS48,\"1/16/2020 5 a.m. EST\",378562,378398,370227,3585\\nUS48,\"1/16/2020 6 a.m. EST\",388591,395500,384456,3633\\nUS48,\"1/16/2020 7 a.m. EST\",415832,422221,409464,4049\\nUS48,\"1/16/2020 8 a.m. EST\",444906,446143,431596,4385\\nUS48,\"1/16/2020 9 a.m. EST\",460077,459092,442993,3918\\nUS48,\"1/16/2020 10 a.m. EST\",467868,465944,450332,3690\\nUS48,\"1/16/2020 11 a.m. EST\",470695,468643,453210,4610\\nUS48,\"1/16/2020 12 p.m. EST\",469098,467089,456565,3935\\nUS48,\"1/16/2020 1 p.m. EST\",466975,463873,447874,4067\\nUS48,\"1/16/2020 2 p.m. EST\",465834,461117,451952,4409\\nUS48,\"1/16/2020 3 p.m. EST\",464640,459333,449061,4014\\nUS48,\"1/16/2020 4 p.m. EST\",463291,460113,450851,4575\\nUS48,\"1/16/2020 5 p.m. EST\",467242,465923,458895,4375\\nUS48,\"1/16/2020 6 p.m. EST\",478388,474473,467337,4650\\nUS48,\"1/16/2020 7 p.m. EST\",494333,485605,478102,4243\\nUS48,\"1/16/2020 8 p.m. EST\",500163,490752,480858,3976\\nUS48,\"1/16/2020 9 p.m. EST\",500130,489351,477930,3496\\nUS48,\"1/16/2020 10 p.m. EST\",489860,477750,467695,3543\\nUS48,\"1/16/2020 11 p.m. EST\",471619,459509,452385,3449\\nUS48,\"1/17/2020 12 a.m. EST\",452181,439536,423538,3335\\nUS48,\"1/17/2020 1 a.m. EST\",433846,422052,418267,3426\\nUS48,\"1/17/2020 2 a.m. EST\",421202,380746,414561,5184\\nUS48,\"1/17/2020 3 a.m. EST\",413935,375901,405237,4991\\nUS48,\"1/17/2020 4 a.m. EST\",411358,366957,403435,4380\\nUS48,\"1/17/2020 5 a.m. EST\",413997,374773,408271,4780\\nUS48,\"1/17/2020 6 a.m. EST\",426062,393458,425060,5340\\nUS48,\"1/17/2020 7 a.m. EST\",453110,419712,451322,6048\\nUS48,\"1/17/2020 8 a.m. EST\",479239,439736,473428,6235\\nUS48,\"1/17/2020 9 a.m. EST\",490382,447931,481274,5779\\nUS48,\"1/17/2020 10 a.m. EST\",493854,449424,483557,5204\\nUS48,\"1/17/2020 11 a.m. EST\",493711,447301,483216,4587\\nUS48,\"1/17/2020 12 p.m. EST\",487661,440762,479145,5065\\nUS48,\"1/17/2020 1 p.m. EST\",480618,433545,471638,4494\\nUS48,\"1/17/2020 2 p.m. EST\",473928,427174,466370,4044\\nUS48,\"1/17/2020 3 p.m. EST\",469032,423021,464051,4420\\nUS48,\"1/17/2020 4 p.m. EST\",466812,422537,462432,4718\\nUS48,\"1/17/2020 5 p.m. EST\",469378,428662,469764,4871\\nUS48,\"1/17/2020 6 p.m. EST\",480058,435637,477613,4928\\nUS48,\"1/17/2020 7 p.m. EST\",491120,443903,484842,5788\\nUS48,\"1/17/2020 8 p.m. EST\",493264,445551,484976,3933\\nUS48,\"1/17/2020 9 p.m. EST\",490031,442951,481725,3391\\nUS48,\"1/17/2020 10 p.m. EST\",480364,433526,469279,3414\\nUS48,\"1/17/2020 11 p.m. EST\",466193,419140,454255,3303\\nUS48,\"1/18/2020 12 a.m. EST\",449235,401839,434138,3199\\nUS48,\"1/18/2020 1 a.m. EST\",433455,388656,421878,2270\\nUS48,\"1/18/2020 2 a.m. EST\",420028,372626,409100,2355\\nUS48,\"1/18/2020 3 a.m. EST\",411290,365135,399490,1926\\nUS48,\"1/18/2020 4 a.m. EST\",403807,361397,392853,1840\\nUS48,\"1/18/2020 5 a.m. EST\",401604,362077,391554,1805\\nUS48,\"1/18/2020 6 a.m. EST\",403735,367773,395401,2049\\nUS48,\"1/18/2020 7 a.m. EST\",411079,378200,402849,2093\\nUS48,\"1/18/2020 8 a.m. EST\",424436,390290,417573,2553\\nUS48,\"1/18/2020 9 a.m. EST\",439066,402458,431115,3460\\nUS48,\"1/18/2020 10 a.m. EST\",451145,410937,443886,3098\\nUS48,\"1/18/2020 11 a.m. EST\",459660,413546,452103,3525\\nUS48,\"1/18/2020 12 p.m. EST\",461354,410803,454531,4647\\nUS48,\"1/18/2020 1 p.m. EST\",459199,405154,451092,5119\\nUS48,\"1/18/2020 2 p.m. EST\",454938,399495,447161,5827\\nUS48,\"1/18/2020 3 p.m. EST\",449758,395506,443544,5781\\nUS48,\"1/18/2020 4 p.m. EST\",446429,394950,440793,5971\\nUS48,\"1/18/2020 5 p.m. EST\",447368,398538,445400,6110\\nUS48,\"1/18/2020 6 p.m. EST\",473412,404974,448718,6100\\nUS48,\"1/18/2020 7 p.m. EST\",443344,412539,453812,5362\\nUS48,\"1/18/2020 8 p.m. EST\",464828,414127,454741,5014\\nUS48,\"1/18/2020 9 p.m. EST\",462589,412109,449293,3377\\nUS48,\"1/18/2020 10 p.m. EST\",453773,403453,441626,2857\\nUS48,\"1/18/2020 11 p.m. EST\",442659,390545,429944,2445\\nUS48,\"1/19/2020 12 a.m. EST\",448545,377311,424081,3058\\nUS48,\"1/19/2020 1 a.m. EST\",415392,366128,404352,3040\\nUS48,\"1/19/2020 2 a.m. EST\",406575,393819,395701,3024\\nUS48,\"1/19/2020 3 a.m. EST\",399181,387163,388859,3028\\nUS48,\"1/19/2020 4 a.m. EST\",395031,385568,386262,2497\\nUS48,\"1/19/2020 5 a.m. EST\",395321,387924,386332,2502\\nUS48,\"1/19/2020 6 a.m. EST\",397897,395113,390421,2386\\nUS48,\"1/19/2020 7 a.m. EST\",407317,406588,400497,2603\\nUS48,\"1/19/2020 8 a.m. EST\",420885,422149,414908,2442\\nUS48,\"1/19/2020 9 a.m. EST\",437903,437758,430369,2238\\nUS48,\"1/19/2020 10 a.m. EST\",450853,448011,441232,1938\\nUS48,\"1/19/2020 11 a.m. EST\",453333,449959,443947,2698\\nUS48,\"1/19/2020 12 p.m. EST\",450411,447103,441130,2790\\nUS48,\"1/19/2020 1 p.m. EST\",446021,442747,437446,3156\\nUS48,\"1/19/2020 2 p.m. EST\",441024,437561,432136,2950\\nUS48,\"1/19/2020 3 p.m. EST\",437243,433406,428784,3306\\nUS48,\"1/19/2020 4 p.m. EST\",435576,433123,428925,3396\\nUS48,\"1/19/2020 5 p.m. EST\",441111,440722,436943,3650\\nUS48,\"1/19/2020 6 p.m. EST\",457449,454606,449841,2367\\nUS48,\"1/19/2020 7 p.m. EST\",476716,472078,465921,2145\\nUS48,\"1/19/2020 8 p.m. EST\",487422,481477,475861,2044\\nUS48,\"1/19/2020 9 p.m. EST\",491662,483401,477096,921\\nUS48,\"1/19/2020 10 p.m. EST\",485619,477366,471310,1944\\nUS48,\"1/19/2020 11 p.m. EST\",475857,465917,461410,2389\\nUS48,\"1/20/2020 12 a.m. EST\",461223,451050,439557,2738\\nUS48,\"1/20/2020 1 a.m. EST\",449412,439423,,\\nUS48,\"1/20/2020 2 a.m. EST\",442192,433385,,\\nUS48,\"1/20/2020 3 a.m. EST\",438007,430647,,\\nUS48,\"1/20/2020 4 a.m. EST\",437428,434272,,\\nUS48,\"1/20/2020 5 a.m. EST\",442598,444052,,\\nUS48,\"1/20/2020 6 a.m. EST\",456569,462500,,\\nUS48,\"1/20/2020 7 a.m. EST\",480598,487170,,\\nUS48,\"1/20/2020 8 a.m. EST\",505034,509542,,\\nUS48,\"1/20/2020 9 a.m. EST\",520114,521154,,\\nUS48,\"1/20/2020 10 a.m. EST\",525757,521796,,\\nUS48,\"1/20/2020 11 a.m. EST\",523017,515462,,\\nUS48,\"1/20/2020 12 p.m. EST\",516270,506293,,\\nUS48,\"1/20/2020 1 p.m. EST\",506637,494807,,\\nUS48,\"1/20/2020 2 p.m. EST\",,485316,,\\nUS48,\"1/20/2020 3 p.m. EST\",,477848,,\\nUS48,\"1/20/2020 4 p.m. EST\",,475395,,\\nUS48,\"1/20/2020 5 p.m. EST\",,482383,,\\nUS48,\"1/20/2020 6 p.m. EST\",,496691,,\\nUS48,\"1/20/2020 7 p.m. EST\",,515522,,\\nUS48,\"1/20/2020 8 p.m. EST\",,525340,,\\nUS48,\"1/20/2020 9 p.m. EST\",,525704,,\\nUS48,\"1/20/2020 10 p.m. EST\",,515812,,\\nUS48,\"1/20/2020 11 p.m. EST\",,498895,,\\nUS48,\"1/21/2020 12 a.m. EST\",,481688,,\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PszKmVvgIbG",
        "colab_type": "code",
        "outputId": "611beadc-86c1-4227-d76c-e250c2d83e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        " electricity = pd.read_csv('930-data-export.csv')\n",
        " print(electricity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Region Code  ... Total Interchange (MWh)\n",
            "0          US48  ...                  2977.0\n",
            "1          US48  ...                  3557.0\n",
            "2          US48  ...                  3755.0\n",
            "3          US48  ...                  3617.0\n",
            "4          US48  ...                  3968.0\n",
            "..          ...  ...                     ...\n",
            "188        US48  ...                     NaN\n",
            "189        US48  ...                     NaN\n",
            "190        US48  ...                     NaN\n",
            "191        US48  ...                     NaN\n",
            "192        US48  ...                     NaN\n",
            "\n",
            "[193 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RywEUlRkJ7W4",
        "colab_type": "text"
      },
      "source": [
        "https://www.eia.gov/beta/electricity/gridmonitor/dashboard/electric_overview/US48/US48?src=email"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLvB2yxOML9P",
        "colab_type": "code",
        "outputId": "dc2f0536-2d50-45d0-c6ad-089b79c31b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "\n",
        "Data_to_predict = quandl.get(\"FRBP/GDPPLUS_042619\",  collapse=\"quarterly\")\n",
        "datafive = quandl.get(\"FRED/PCETRIM1M158SFRBDAL\",  collapse=\"quarterly\", start_date=\"1977-02-01\",end_date=\"2016-03-31\")#quandl.get(\"FRED/VALEXPUSM052N\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\", collapse=\"quarterly\", start_date=\"1960-09-30\")#quandl.get(\"WWDI/USA_NE_GDI_TOTL_CD\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", start_date=\"1970-12-31\")\n",
        "Data_To_predict = Data_to_predict.values\n",
        "#the below column reduces the dimension of the data set by 1. We need to do this sometimes in order to put it into a pandas dataframe which we can easily manipulate\n",
        "\n",
        "electricity= pd.DataFrame(electricity)\n",
        "datafive= pd.DataFrame(datafive)\n",
        "\n",
        "plt.plot(datafive)\n",
        "print(datafive)\n",
        "print(electricity)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LimitExceededError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLimitExceededError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ffc4549e78d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mData_to_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FRBP/GDPPLUS_042619\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcollapse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"quarterly\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdatafive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FRED/PCETRIM1M158SFRBDAL\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcollapse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"quarterly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1977-02-01\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2016-03-31\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#quandl.get(\"FRED/VALEXPUSM052N\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\", collapse=\"quarterly\", start_date=\"1960-09-30\")#quandl.get(\"WWDI/USA_NE_GDI_TOTL_CD\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", start_date=\"1970-12-31\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mData_To_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_to_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#the below column reduces the dimension of the data set by 1. We need to do this sometimes in order to put it into a pandas dataframe which we can easily manipulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/get.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dataset, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_column_not_found\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/model/dataset.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, **options)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mupdated_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mupdated_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandle_not_found_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/operations/list.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(cls, **options)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructed_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mabs_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_verb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mexecute_request\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     48\u001b[0m                                        **options)\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_api_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mhandle_api_error\u001b[0;34m(cls, resp)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_klass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_letter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuandlError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mLimitExceededError\u001b[0m: (Status 429) (Quandl Error QELx01) You have exceeded the anonymous user limit of 50 calls per day. To make more calls today, please register for a free Quandl account and then include your API key with your requests."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TItxs-oC31ud",
        "colab_type": "code",
        "outputId": "0651c411-ac78-41e2-8e34-55a1f18836f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Standardize the data\n",
        "\n",
        "#note on minmaxScalar vs standard scalar using standard scalar is generally inadvisable because of the fact that standard scalar tries to force the data to conform to a standard normal distribution. if the data does not conform to a\n",
        "#standard normal distribution this can end up distorting the data. \n",
        "Scalar = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "b = Scalar.fit_transform(datafive)\n",
        "\n",
        "#b = np.log10(b)\n",
        "\n",
        "\n",
        "plt.plot(datafive)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.64537445]\n",
            " [0.6222467 ]\n",
            " [0.47907489]\n",
            " [0.54515419]\n",
            " [0.61123348]\n",
            " [0.64537445]\n",
            " [0.71696035]\n",
            " [0.62665198]\n",
            " [0.66079295]\n",
            " [0.70484581]\n",
            " [0.86894273]\n",
            " [0.8314978 ]\n",
            " [0.97907489]\n",
            " [0.91519824]\n",
            " [1.        ]\n",
            " [0.83259912]\n",
            " [0.73348018]\n",
            " [0.58810573]\n",
            " [0.74669604]\n",
            " [0.48898678]\n",
            " [0.49779736]\n",
            " [0.54405286]\n",
            " [0.27643172]\n",
            " [0.26321586]\n",
            " [0.18832599]\n",
            " [0.30837004]\n",
            " [0.39757709]\n",
            " [0.20594714]\n",
            " [0.38325991]\n",
            " [0.31828194]\n",
            " [0.3777533 ]\n",
            " [0.35132159]\n",
            " [0.37885463]\n",
            " [0.39757709]\n",
            " [0.32709251]\n",
            " [0.36123348]\n",
            " [0.12555066]\n",
            " [0.37334802]\n",
            " [0.34911894]\n",
            " [0.2753304 ]\n",
            " [0.37555066]\n",
            " [0.29515419]\n",
            " [0.31277533]\n",
            " [0.14647577]\n",
            " [0.35462555]\n",
            " [0.43281938]\n",
            " [0.52312775]\n",
            " [0.33590308]\n",
            " [0.36784141]\n",
            " [0.28193833]\n",
            " [0.31497797]\n",
            " [0.3623348 ]\n",
            " [0.53744493]\n",
            " [0.46475771]\n",
            " [0.30396476]\n",
            " [0.30726872]\n",
            " [0.15198238]\n",
            " [0.17070485]\n",
            " [0.28634361]\n",
            " [0.34581498]\n",
            " [0.26651982]\n",
            " [0.17290749]\n",
            " [0.10022026]\n",
            " [0.20704846]\n",
            " [0.18502203]\n",
            " [0.12555066]\n",
            " [0.16079295]\n",
            " [0.17180617]\n",
            " [0.26101322]\n",
            " [0.21475771]\n",
            " [0.1277533 ]\n",
            " [0.12555066]\n",
            " [0.20704846]\n",
            " [0.1123348 ]\n",
            " [0.14647577]\n",
            " [0.17621145]\n",
            " [0.21475771]\n",
            " [0.16079295]\n",
            " [0.22246696]\n",
            " [0.16189427]\n",
            " [0.14757709]\n",
            " [0.15418502]\n",
            " [0.20374449]\n",
            " [0.06057269]\n",
            " [0.20594714]\n",
            " [0.06718062]\n",
            " [0.13546256]\n",
            " [0.10242291]\n",
            " [0.10462555]\n",
            " [0.09361233]\n",
            " [0.1938326 ]\n",
            " [0.14537445]\n",
            " [0.25110132]\n",
            " [0.1685022 ]\n",
            " [0.280837  ]\n",
            " [0.18722467]\n",
            " [0.20044053]\n",
            " [0.28964758]\n",
            " [0.1277533 ]\n",
            " [0.08039648]\n",
            " [0.18832599]\n",
            " [0.15638767]\n",
            " [0.21365639]\n",
            " [0.11123348]\n",
            " [0.15638767]\n",
            " [0.04295154]\n",
            " [0.14317181]\n",
            " [0.12444934]\n",
            " [0.1685022 ]\n",
            " [0.20154185]\n",
            " [0.13986784]\n",
            " [0.14647577]\n",
            " [0.17621145]\n",
            " [0.11123348]\n",
            " [0.28414097]\n",
            " [0.11674009]\n",
            " [0.24008811]\n",
            " [0.33590308]\n",
            " [0.16079295]\n",
            " [0.26321586]\n",
            " [0.15528634]\n",
            " [0.21035242]\n",
            " [0.27643172]\n",
            " [0.22136564]\n",
            " [0.23237885]\n",
            " [0.27422907]\n",
            " [0.18832599]\n",
            " [0.05396476]\n",
            " [0.11123348]\n",
            " [0.09471366]\n",
            " [0.02973568]\n",
            " [0.00660793]\n",
            " [0.00110132]\n",
            " [0.        ]\n",
            " [0.06718062]\n",
            " [0.06718062]\n",
            " [0.17511013]\n",
            " [0.08810573]\n",
            " [0.12885463]\n",
            " [0.1530837 ]\n",
            " [0.17070485]\n",
            " [0.06057269]\n",
            " [0.13546256]\n",
            " [0.04845815]\n",
            " [0.10022026]\n",
            " [0.18502203]\n",
            " [0.12004405]\n",
            " [0.1222467 ]\n",
            " [0.13876652]\n",
            " [0.12665198]\n",
            " [0.13325991]\n",
            " [0.08370044]\n",
            " [0.1376652 ]\n",
            " [0.15859031]\n",
            " [0.11453744]\n",
            " [0.05506608]\n",
            " [0.13656388]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD6CAYAAAC4RRw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntCn/P4095MpUuad7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o3bVkti34kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#filling and eliminating missing data filling and eliminating missing data\n",
        "#I think this one is about dropping the time index column from the data\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats.mstats import winsorize\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "final_data = imp.fit_transform(datafive)\n",
        "#print(a)\n",
        "#datasix = datasix.drop(datasix.index[0])\n",
        "#Data_to_predict.values\n",
        "#datasix = datasix.dropna()\n",
        "#datafive = datafive.fillna(0)\n",
        "#unfortunately dropna() and fillna(0) are not very good solutions what if there were a better way?\n",
        "pca = PCA()# n_components=2 are the components in the dataset which we want to keep which have the greatest amount of correlation\n",
        "final_data = pca.fit_transform(final_data)\n",
        "\n",
        "#winsorization is for getting rid of outlier values in the data \n",
        "final_data = \twinsorize(final_data)\n",
        "\n",
        "\n",
        "final_data = pd.DataFrame(final_data)\n",
        "\n",
        "#this is basically selecting the first 224 values starting from the most initial value\n",
        "final_data_sub = final_data[224::]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2mX2tCtO4gp",
        "colab_type": "code",
        "outputId": "db838b0c-806c-4a73-d7a9-8a3ac7e91a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "#what if we want to use two or more datasets in order to predict values in the third dataset? or we want to a dataset which says what category that dataset is in? we can use pd.concat for that \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "final_data= pd.concat([final_data,electricity],axis=1)# what this does is make sure that the datasets are connectet horizontally. so the other dataset basically becomes a second column in the new consolidated dataset. \n",
        "\n",
        "#if you want to do the splitting before putting it into a deep learning neural network then train_test_split is what is needed\n",
        "#whatever your y= will go here \n",
        "X_train, X_test, y_train, y_test = train_test_split(final_data, y, test_size=0.33, random_state=42)\n",
        "#.pca\n",
        "#you HAVE to convert your pandas dataframes into arrays before you can feed it into any ml or deep learning neural network or algorithm\n",
        "#higher order derivative(s) of values are easier for a machine learning algorithm to predict then the actual values\n",
        "electricity = np.array(electricity)\n",
        "#electricity = np.diff(electricity)\n",
        "final_data = np.array(final_data)\n",
        "final_data = np.diff(final_data)\n",
        "electricity = np.squeeze(electricity,axis=1)\n",
        "\n",
        "#generally you will only need to expand the number of dimensions of the dataset when you are working with RNNs or recurrent neural networks. \n",
        "electricity = np.expand_dims(electricity,axis=1)\n",
        "#this is dummy y data\n",
        "y = np.zeros(256)\n",
        "y = np.array(y)\n",
        "\n",
        "#again only something you are going to want to do if you are working with Recurrent neural networks\n",
        "X[:, None] \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5e80a983f296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melectricity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# what this does is make sure that the datasets are connectet horizontally. so the other dataset basically becomes a second column in the new consolidated dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#.pca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0;34m\" only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 )\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRBu-Ha54oNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "c3e1b4bc-216c-4d5a-aa39-96f14147e8e1"
      },
      "source": [
        "#the above is generally not needed however because tensorflow allows you to split it during the fitting phase and once can simply manually slice\n",
        "#the arrays \n",
        "#model.fit(X,Y,epochs=700, verbose=1,validation_split=0.2)\n",
        "\n",
        "final_data = np.squeeze(final_data)\n",
        "plt.plot(final_data)\n",
        "#if enough time reamains go into various data visualization such as data decomposition, graphing and confidence intervals sns library. "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-49e4ad4f59b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#if enough time reamains go into various data visualization such as data decomposition, graphing and confidence intervals sns library.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2uGhlfEByPq",
        "colab_type": "text"
      },
      "source": [
        "Part two starts from here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYRgvzpgB0xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 732
        },
        "outputId": "5508c36f-522f-4196-a872-41d88df3ce84"
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1\n",
        "#!pip install tensorflow==2.0.0rc0\n",
        "#!pip install tensorflow-probability\n",
        "#!pip install gaussian_processes\n",
        "#!pip install quandl\n",
        "#!pip install rpy2\n",
        "#!pip install bootstrapped\n",
        "!pip install hurst"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n",
            "\u001b[K     || 87.9MB 62kB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.17.5)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     || 501kB 51.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.34.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.10.0)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     || 3.1MB 41.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.27.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.9.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==2.0.0-beta1) (45.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (1.0.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n",
            "Installing collected packages: tf-estimator-nightly, tb-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n",
            "Collecting hurst\n",
            "  Downloading https://files.pythonhosted.org/packages/02/4f/d3471ce0dca03a21d4c6640da07a6040c9cc800a937233086b6cea6a7dc2/hurst-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hurst) (1.17.5)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.6/dist-packages (from hurst) (0.25.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->hurst) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->hurst) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18->hurst) (1.12.0)\n",
            "Installing collected packages: hurst\n",
            "Successfully installed hurst-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vP7mnY6B4rr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "outputId": "85192983-e49f-4b5c-bcba-41487ef31122"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#import quandl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "#import sherpa"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92kvs2q9VxjV",
        "colab_type": "text"
      },
      "source": [
        "Now we will do classifcation in the functional instead of sequential form. The functional form is considered more powerful because it among other things allows for more flexibility such as connecting layers at different levels to on another. \n",
        "\n",
        "Note: I use MSLE and ADagrad because those are the optimizers and losses which work best for me. However the standard \"default optimizer and loss to use is MSE and ADAM respectively for regression and binary crossentropy for 2 category classifcation algorithms and categorical crossentropy as the default loss function for classification algorithms with more then two buckets\n",
        "\n",
        "Take a look at the last layer of the neural network. One of the things which must be kept in mind is that the number of nodes in a neural netowork of the last layer MUST equal the number of buckets for our data. Also a sigmoid activation function is generally the standard activation function to use in the final layer of classifcation neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUDKBHkasbSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9e936504-236c-406f-febf-2c303017945c"
      },
      "source": [
        "from sklearn.datasets import load_iris, fetch_california_housing\n",
        "\n",
        "data = load_iris()\n",
        "print(data)\n",
        "print(data['target'])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
            "       [4.9, 3. , 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.3, 0.2],\n",
            "       [4.6, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.6, 1.4, 0.2],\n",
            "       [5.4, 3.9, 1.7, 0.4],\n",
            "       [4.6, 3.4, 1.4, 0.3],\n",
            "       [5. , 3.4, 1.5, 0.2],\n",
            "       [4.4, 2.9, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.1],\n",
            "       [5.4, 3.7, 1.5, 0.2],\n",
            "       [4.8, 3.4, 1.6, 0.2],\n",
            "       [4.8, 3. , 1.4, 0.1],\n",
            "       [4.3, 3. , 1.1, 0.1],\n",
            "       [5.8, 4. , 1.2, 0.2],\n",
            "       [5.7, 4.4, 1.5, 0.4],\n",
            "       [5.4, 3.9, 1.3, 0.4],\n",
            "       [5.1, 3.5, 1.4, 0.3],\n",
            "       [5.7, 3.8, 1.7, 0.3],\n",
            "       [5.1, 3.8, 1.5, 0.3],\n",
            "       [5.4, 3.4, 1.7, 0.2],\n",
            "       [5.1, 3.7, 1.5, 0.4],\n",
            "       [4.6, 3.6, 1. , 0.2],\n",
            "       [5.1, 3.3, 1.7, 0.5],\n",
            "       [4.8, 3.4, 1.9, 0.2],\n",
            "       [5. , 3. , 1.6, 0.2],\n",
            "       [5. , 3.4, 1.6, 0.4],\n",
            "       [5.2, 3.5, 1.5, 0.2],\n",
            "       [5.2, 3.4, 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.6, 0.2],\n",
            "       [4.8, 3.1, 1.6, 0.2],\n",
            "       [5.4, 3.4, 1.5, 0.4],\n",
            "       [5.2, 4.1, 1.5, 0.1],\n",
            "       [5.5, 4.2, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.2, 1.2, 0.2],\n",
            "       [5.5, 3.5, 1.3, 0.2],\n",
            "       [4.9, 3.6, 1.4, 0.1],\n",
            "       [4.4, 3. , 1.3, 0.2],\n",
            "       [5.1, 3.4, 1.5, 0.2],\n",
            "       [5. , 3.5, 1.3, 0.3],\n",
            "       [4.5, 2.3, 1.3, 0.3],\n",
            "       [4.4, 3.2, 1.3, 0.2],\n",
            "       [5. , 3.5, 1.6, 0.6],\n",
            "       [5.1, 3.8, 1.9, 0.4],\n",
            "       [4.8, 3. , 1.4, 0.3],\n",
            "       [5.1, 3.8, 1.6, 0.2],\n",
            "       [4.6, 3.2, 1.4, 0.2],\n",
            "       [5.3, 3.7, 1.5, 0.2],\n",
            "       [5. , 3.3, 1.4, 0.2],\n",
            "       [7. , 3.2, 4.7, 1.4],\n",
            "       [6.4, 3.2, 4.5, 1.5],\n",
            "       [6.9, 3.1, 4.9, 1.5],\n",
            "       [5.5, 2.3, 4. , 1.3],\n",
            "       [6.5, 2.8, 4.6, 1.5],\n",
            "       [5.7, 2.8, 4.5, 1.3],\n",
            "       [6.3, 3.3, 4.7, 1.6],\n",
            "       [4.9, 2.4, 3.3, 1. ],\n",
            "       [6.6, 2.9, 4.6, 1.3],\n",
            "       [5.2, 2.7, 3.9, 1.4],\n",
            "       [5. , 2. , 3.5, 1. ],\n",
            "       [5.9, 3. , 4.2, 1.5],\n",
            "       [6. , 2.2, 4. , 1. ],\n",
            "       [6.1, 2.9, 4.7, 1.4],\n",
            "       [5.6, 2.9, 3.6, 1.3],\n",
            "       [6.7, 3.1, 4.4, 1.4],\n",
            "       [5.6, 3. , 4.5, 1.5],\n",
            "       [5.8, 2.7, 4.1, 1. ],\n",
            "       [6.2, 2.2, 4.5, 1.5],\n",
            "       [5.6, 2.5, 3.9, 1.1],\n",
            "       [5.9, 3.2, 4.8, 1.8],\n",
            "       [6.1, 2.8, 4. , 1.3],\n",
            "       [6.3, 2.5, 4.9, 1.5],\n",
            "       [6.1, 2.8, 4.7, 1.2],\n",
            "       [6.4, 2.9, 4.3, 1.3],\n",
            "       [6.6, 3. , 4.4, 1.4],\n",
            "       [6.8, 2.8, 4.8, 1.4],\n",
            "       [6.7, 3. , 5. , 1.7],\n",
            "       [6. , 2.9, 4.5, 1.5],\n",
            "       [5.7, 2.6, 3.5, 1. ],\n",
            "       [5.5, 2.4, 3.8, 1.1],\n",
            "       [5.5, 2.4, 3.7, 1. ],\n",
            "       [5.8, 2.7, 3.9, 1.2],\n",
            "       [6. , 2.7, 5.1, 1.6],\n",
            "       [5.4, 3. , 4.5, 1.5],\n",
            "       [6. , 3.4, 4.5, 1.6],\n",
            "       [6.7, 3.1, 4.7, 1.5],\n",
            "       [6.3, 2.3, 4.4, 1.3],\n",
            "       [5.6, 3. , 4.1, 1.3],\n",
            "       [5.5, 2.5, 4. , 1.3],\n",
            "       [5.5, 2.6, 4.4, 1.2],\n",
            "       [6.1, 3. , 4.6, 1.4],\n",
            "       [5.8, 2.6, 4. , 1.2],\n",
            "       [5. , 2.3, 3.3, 1. ],\n",
            "       [5.6, 2.7, 4.2, 1.3],\n",
            "       [5.7, 3. , 4.2, 1.2],\n",
            "       [5.7, 2.9, 4.2, 1.3],\n",
            "       [6.2, 2.9, 4.3, 1.3],\n",
            "       [5.1, 2.5, 3. , 1.1],\n",
            "       [5.7, 2.8, 4.1, 1.3],\n",
            "       [6.3, 3.3, 6. , 2.5],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [7.1, 3. , 5.9, 2.1],\n",
            "       [6.3, 2.9, 5.6, 1.8],\n",
            "       [6.5, 3. , 5.8, 2.2],\n",
            "       [7.6, 3. , 6.6, 2.1],\n",
            "       [4.9, 2.5, 4.5, 1.7],\n",
            "       [7.3, 2.9, 6.3, 1.8],\n",
            "       [6.7, 2.5, 5.8, 1.8],\n",
            "       [7.2, 3.6, 6.1, 2.5],\n",
            "       [6.5, 3.2, 5.1, 2. ],\n",
            "       [6.4, 2.7, 5.3, 1.9],\n",
            "       [6.8, 3. , 5.5, 2.1],\n",
            "       [5.7, 2.5, 5. , 2. ],\n",
            "       [5.8, 2.8, 5.1, 2.4],\n",
            "       [6.4, 3.2, 5.3, 2.3],\n",
            "       [6.5, 3. , 5.5, 1.8],\n",
            "       [7.7, 3.8, 6.7, 2.2],\n",
            "       [7.7, 2.6, 6.9, 2.3],\n",
            "       [6. , 2.2, 5. , 1.5],\n",
            "       [6.9, 3.2, 5.7, 2.3],\n",
            "       [5.6, 2.8, 4.9, 2. ],\n",
            "       [7.7, 2.8, 6.7, 2. ],\n",
            "       [6.3, 2.7, 4.9, 1.8],\n",
            "       [6.7, 3.3, 5.7, 2.1],\n",
            "       [7.2, 3.2, 6. , 1.8],\n",
            "       [6.2, 2.8, 4.8, 1.8],\n",
            "       [6.1, 3. , 4.9, 1.8],\n",
            "       [6.4, 2.8, 5.6, 2.1],\n",
            "       [7.2, 3. , 5.8, 1.6],\n",
            "       [7.4, 2.8, 6.1, 1.9],\n",
            "       [7.9, 3.8, 6.4, 2. ],\n",
            "       [6.4, 2.8, 5.6, 2.2],\n",
            "       [6.3, 2.8, 5.1, 1.5],\n",
            "       [6.1, 2.6, 5.6, 1.4],\n",
            "       [7.7, 3. , 6.1, 2.3],\n",
            "       [6.3, 3.4, 5.6, 2.4],\n",
            "       [6.4, 3.1, 5.5, 1.8],\n",
            "       [6. , 3. , 4.8, 1.8],\n",
            "       [6.9, 3.1, 5.4, 2.1],\n",
            "       [6.7, 3.1, 5.6, 2.4],\n",
            "       [6.9, 3.1, 5.1, 2.3],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [6.8, 3.2, 5.9, 2.3],\n",
            "       [6.7, 3.3, 5.7, 2.5],\n",
            "       [6.7, 3. , 5.2, 2.3],\n",
            "       [6.3, 2.5, 5. , 1.9],\n",
            "       [6.5, 3. , 5.2, 2. ],\n",
            "       [6.2, 3.4, 5.4, 2.3],\n",
            "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/iris.csv'}\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7sHudVXFP7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a07bcdc6-488e-4e82-d7be-7382975689a1"
      },
      "source": [
        "#DNN for regression\n",
        "\n",
        "X_full, y_full = fetch_california_housing(return_X_y=True)\n",
        "print(X_full.shape)\n",
        "print(y_full.shape)\n",
        "#X_full = X_full.reshape(-1,1)\n",
        "#y_full = y_full.reshape(-1,1)\n",
        "Scalar = MinMaxScaler(feature_range=(0,1))\n",
        "X_full = Scalar.fit_transform(X_full)\n",
        "#y_full = Scalar.fit_transform(y_full)\n",
        "\n",
        "pca = PCA()# n_components=2 are the components in the dataset which we want to keep which have the greatest amount of correlation\n",
        "#X_full = Scalar.fit_transform(X_full)\n",
        "#y_full = Scalar.fit_transform(y_full)\n",
        "X_full = pca.fit_transform(X_full)\n",
        "#\n",
        "print(X_full)\n",
        "print(X_full.shape)\n",
        "model = tf.keras.Sequential([\n",
        "\ttf.keras.Input(shape=(8,)),\n",
        "  tf.keras.layers.Dense(25,kernel_initializer='ones', use_bias=False),\n",
        "  #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\n",
        "  tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(25,kernel_initializer='ones', use_bias=False)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='Adam',\n",
        "        loss='MSLE')\n",
        "\t\n",
        "model.fit(X_full,y_full,epochs=100, verbose=1,validation_split=0.2)#, callbacks=[early_stop])\n",
        "\n",
        "Prediction = model.predict(X_full[120::])\n",
        "print(Prediction)\n",
        "print(Prediction.shape)\n",
        "u = 219\n",
        "model.summary()\n",
        "\n",
        "plt.plot(Prediction)\n",
        "plt.plot(y_full[120::])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n",
            "[[ 3.77687550e-01 -1.64446431e-01  3.33406202e-01 ... -3.76119361e-03\n",
            "  -6.03179739e-04  2.08036107e-03]\n",
            " [ 3.16757787e-01  2.22548375e-01  3.02566145e-01 ... -5.51572134e-03\n",
            "  -1.65845618e-03  4.36229044e-03]\n",
            " [ 4.09871621e-01 -3.82419974e-01  2.77266167e-01 ...  9.09329494e-03\n",
            "  -8.05730847e-04 -5.69533618e-03]\n",
            " ...\n",
            " [ 3.77714090e-01  2.82229585e-01 -1.78309032e-01 ... -1.39043799e-02\n",
            "  -4.57164294e-04  9.06260258e-05]\n",
            " [ 3.86895154e-01  2.64153572e-01 -1.64186327e-01 ... -1.25170303e-02\n",
            "  -4.41388751e-04  9.67480249e-04]\n",
            " [ 3.69856553e-01  3.04781910e-01 -1.32269193e-01 ... -1.32333234e-02\n",
            "  -4.90778420e-04  2.36978974e-03]]\n",
            "(20640, 8)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 16512 samples, validate on 4128 samples\n",
            "Epoch 1/100\n",
            "16512/16512 [==============================] - 1s 77us/sample - loss: 2.3851 - val_loss: 1.5991\n",
            "Epoch 2/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 1.1387 - val_loss: 0.9713\n",
            "Epoch 3/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 1.0251 - val_loss: 1.3691\n",
            "Epoch 4/100\n",
            "16512/16512 [==============================] - 1s 70us/sample - loss: 0.5051 - val_loss: 0.2255\n",
            "Epoch 5/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.1827 - val_loss: 0.0842\n",
            "Epoch 6/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0609 - val_loss: 0.0442\n",
            "Epoch 7/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0574 - val_loss: 0.0431\n",
            "Epoch 8/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0563 - val_loss: 0.0439\n",
            "Epoch 9/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0545 - val_loss: 0.0430\n",
            "Epoch 10/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0546 - val_loss: 0.0436\n",
            "Epoch 11/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0541 - val_loss: 0.0443\n",
            "Epoch 12/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0537 - val_loss: 0.0440\n",
            "Epoch 13/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0528 - val_loss: 0.0434\n",
            "Epoch 14/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0525 - val_loss: 0.0433\n",
            "Epoch 15/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0520 - val_loss: 0.0437\n",
            "Epoch 16/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0523 - val_loss: 0.0440\n",
            "Epoch 17/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0528 - val_loss: 0.0452\n",
            "Epoch 18/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0521 - val_loss: 0.0433\n",
            "Epoch 19/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0518 - val_loss: 0.0436\n",
            "Epoch 20/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0523 - val_loss: 0.0432\n",
            "Epoch 21/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0519 - val_loss: 0.0438\n",
            "Epoch 22/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0523 - val_loss: 0.0439\n",
            "Epoch 23/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0526 - val_loss: 0.0458\n",
            "Epoch 24/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0520 - val_loss: 0.0438\n",
            "Epoch 25/100\n",
            "16512/16512 [==============================] - 1s 70us/sample - loss: 0.0522 - val_loss: 0.0463\n",
            "Epoch 26/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0514 - val_loss: 0.0460\n",
            "Epoch 27/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0519 - val_loss: 0.0446\n",
            "Epoch 28/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0511 - val_loss: 0.0460\n",
            "Epoch 29/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0510 - val_loss: 0.0454\n",
            "Epoch 30/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0517 - val_loss: 0.0435\n",
            "Epoch 31/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0513 - val_loss: 0.0470\n",
            "Epoch 32/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0512 - val_loss: 0.0444\n",
            "Epoch 33/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0512 - val_loss: 0.0424\n",
            "Epoch 34/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0514 - val_loss: 0.0432\n",
            "Epoch 35/100\n",
            "16512/16512 [==============================] - 1s 71us/sample - loss: 0.0513 - val_loss: 0.0426\n",
            "Epoch 36/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0510 - val_loss: 0.0427\n",
            "Epoch 37/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0508 - val_loss: 0.0483\n",
            "Epoch 38/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0509 - val_loss: 0.0429\n",
            "Epoch 39/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0508 - val_loss: 0.0432\n",
            "Epoch 40/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0505 - val_loss: 0.0423\n",
            "Epoch 41/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0511 - val_loss: 0.0437\n",
            "Epoch 42/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0514 - val_loss: 0.0446\n",
            "Epoch 43/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0504 - val_loss: 0.0447\n",
            "Epoch 44/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0502 - val_loss: 0.0544\n",
            "Epoch 45/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0511 - val_loss: 0.0434\n",
            "Epoch 46/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0507 - val_loss: 0.0471\n",
            "Epoch 47/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0504 - val_loss: 0.0455\n",
            "Epoch 48/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0507 - val_loss: 0.0472\n",
            "Epoch 49/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0508 - val_loss: 0.0432\n",
            "Epoch 50/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0502 - val_loss: 0.0424\n",
            "Epoch 51/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0502 - val_loss: 0.0454\n",
            "Epoch 52/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0508 - val_loss: 0.0425\n",
            "Epoch 53/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0509 - val_loss: 0.0443\n",
            "Epoch 54/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0504 - val_loss: 0.0546\n",
            "Epoch 55/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0508 - val_loss: 0.0492\n",
            "Epoch 56/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0507 - val_loss: 0.0490\n",
            "Epoch 57/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0502 - val_loss: 0.0578\n",
            "Epoch 58/100\n",
            "16512/16512 [==============================] - 1s 72us/sample - loss: 0.0505 - val_loss: 0.0469\n",
            "Epoch 59/100\n",
            "16512/16512 [==============================] - 1s 72us/sample - loss: 0.0503 - val_loss: 0.0435\n",
            "Epoch 60/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0498 - val_loss: 0.0444\n",
            "Epoch 61/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0501 - val_loss: 0.0444\n",
            "Epoch 62/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0501 - val_loss: 0.0440\n",
            "Epoch 63/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0505 - val_loss: 0.0459\n",
            "Epoch 64/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0508 - val_loss: 0.0566\n",
            "Epoch 65/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0504 - val_loss: 0.0559\n",
            "Epoch 66/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0501 - val_loss: 0.0565\n",
            "Epoch 67/100\n",
            "16512/16512 [==============================] - 1s 71us/sample - loss: 0.0495 - val_loss: 0.0454\n",
            "Epoch 68/100\n",
            "16512/16512 [==============================] - 1s 77us/sample - loss: 0.0501 - val_loss: 0.0428\n",
            "Epoch 69/100\n",
            "16512/16512 [==============================] - 1s 75us/sample - loss: 0.0499 - val_loss: 0.0488\n",
            "Epoch 70/100\n",
            "16512/16512 [==============================] - 1s 74us/sample - loss: 0.0498 - val_loss: 0.0436\n",
            "Epoch 71/100\n",
            "16512/16512 [==============================] - 1s 73us/sample - loss: 0.0505 - val_loss: 0.0539\n",
            "Epoch 72/100\n",
            "16512/16512 [==============================] - 1s 71us/sample - loss: 0.0505 - val_loss: 0.0508\n",
            "Epoch 73/100\n",
            "16512/16512 [==============================] - 1s 71us/sample - loss: 0.0504 - val_loss: 0.0434\n",
            "Epoch 74/100\n",
            "16512/16512 [==============================] - 1s 74us/sample - loss: 0.0501 - val_loss: 0.0449\n",
            "Epoch 75/100\n",
            "16512/16512 [==============================] - 1s 75us/sample - loss: 0.0504 - val_loss: 0.0521\n",
            "Epoch 76/100\n",
            "16512/16512 [==============================] - 1s 73us/sample - loss: 0.0510 - val_loss: 0.0468\n",
            "Epoch 77/100\n",
            "16512/16512 [==============================] - 1s 72us/sample - loss: 0.0499 - val_loss: 0.0427\n",
            "Epoch 78/100\n",
            "16512/16512 [==============================] - 1s 72us/sample - loss: 0.0502 - val_loss: 0.0601\n",
            "Epoch 79/100\n",
            "16512/16512 [==============================] - 1s 74us/sample - loss: 0.0502 - val_loss: 0.0553\n",
            "Epoch 80/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0501 - val_loss: 0.0449\n",
            "Epoch 81/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0503 - val_loss: 0.0430\n",
            "Epoch 82/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0495 - val_loss: 0.0645\n",
            "Epoch 83/100\n",
            "16512/16512 [==============================] - 1s 71us/sample - loss: 0.0502 - val_loss: 0.0486\n",
            "Epoch 84/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0502 - val_loss: 0.0639\n",
            "Epoch 85/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0504 - val_loss: 0.0473\n",
            "Epoch 86/100\n",
            "16512/16512 [==============================] - 1s 69us/sample - loss: 0.0503 - val_loss: 0.0456\n",
            "Epoch 87/100\n",
            "16512/16512 [==============================] - 1s 71us/sample - loss: 0.0506 - val_loss: 0.0624\n",
            "Epoch 88/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0498 - val_loss: 0.0464\n",
            "Epoch 89/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0505 - val_loss: 0.0475\n",
            "Epoch 90/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0500 - val_loss: 0.0432\n",
            "Epoch 91/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0498 - val_loss: 0.0612\n",
            "Epoch 92/100\n",
            "16512/16512 [==============================] - 1s 72us/sample - loss: 0.0497 - val_loss: 0.0457\n",
            "Epoch 93/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0502 - val_loss: 0.0445\n",
            "Epoch 94/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0508 - val_loss: 0.0504\n",
            "Epoch 95/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0503 - val_loss: 0.0431\n",
            "Epoch 96/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0502 - val_loss: 0.0441\n",
            "Epoch 97/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0503 - val_loss: 0.0524\n",
            "Epoch 98/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0498 - val_loss: 0.0447\n",
            "Epoch 99/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0500 - val_loss: 0.0442\n",
            "Epoch 100/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0500 - val_loss: 0.0468\n",
            "[[2.8898494  2.8898494  2.8898482  ... 2.8898492  2.8898497  2.8898497 ]\n",
            " [2.5363371  2.5363371  2.536337   ... 2.5363374  2.5363376  2.536337  ]\n",
            " [2.9244518  2.9244518  2.9244518  ... 2.924452   2.924452   2.9244516 ]\n",
            " ...\n",
            " [0.83680475 0.83680475 0.8368047  ... 0.8368049  0.83680516 0.8368048 ]\n",
            " [0.9675591  0.9675591  0.9675592  ... 0.9675591  0.9675591  0.9675592 ]\n",
            " [1.0829405  1.0829405  1.0829406  ... 1.0829407  1.0829406  1.0829408 ]]\n",
            "(20520, 25)\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_9 (Dense)              (None, 25)                200       \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 50)                1250      \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_11 (Dense)             (None, 25)                1250      \n",
            "=================================================================\n",
            "Total params: 2,900\n",
            "Trainable params: 2,800\n",
            "Non-trainable params: 100\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fc054bb9fd0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0P8BHDn9l6YwIuMAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU1PlXHoGKLW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "271bb0ee-d9d9-4b4e-c9aa-fe6b97adf993"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "import pandas as pd\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "#x = data['data']\n",
        "#print(x.shape)\n",
        "\n",
        "#y = data['target']\n",
        "\n",
        "#print(a)\n",
        "\n",
        "#TODO: construct the output array manually by filling 3 arrays each with 50 0s 50 ones and 50 twos\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "X = Scalar.fit_transform(X)\n",
        "X = pca.fit_transform(X)\n",
        "#y = pd.DataFrame(y)\n",
        "print(X)\n",
        "print(y)\n",
        "from keras.utils import to_categorical\n",
        "y= to_categorical(y)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "#stack y \n",
        "#print(y)\n",
        "#print(y.shape)\n",
        "inputs = tf.keras.Input(shape=(13,))#Input(shape=(1,3))#Debt_data_change_of_change \n",
        "first = tf.keras.layers.Dense(10, activation='relu')(inputs)\n",
        "A = tf.keras.layers.Dense(400, activation='relu')(first)    \n",
        "x = tf.keras.layers.Dense(800, activation='relu')(A)#remeber the prediction probably needs to be shifted back two before it can actually be compared to actual\n",
        "x = tf.keras.layers.Dense(400,activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(10,activation='relu')(x)\n",
        " #\n",
        "outputs = tf.keras.layers.Dense(3, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.RMSprop\n",
        "#Use categorial crossentropy when doing Classification\n",
        "#Use RMSprop as the optimizer in classification\n",
        "model.compile(optimizer='RMSprop',\n",
        "        loss=loss, metrics=['accuracy'])\n",
        "\t\n",
        "model.fit(X,y,epochs=50, verbose=1,validation_split=0.2)#, callbacks=[early_stop])\n",
        "\n",
        "Prediction = model.predict(X)\n",
        "print(Prediction)\n",
        "\n",
        "print(Prediction.shape)\n",
        "u = 219\n",
        "model.summary()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.70633576 -0.25319275  0.02409269 ...  0.04971144 -0.02001158\n",
            "  -0.00872296]\n",
            " [-0.4849768  -0.00882289 -0.28048205 ...  0.07415784 -0.05083753\n",
            "  -0.00524944]\n",
            " [-0.52117227 -0.18918722  0.19621674 ...  0.0121311  -0.10418449\n",
            "  -0.02316441]\n",
            " ...\n",
            " [ 0.62631289 -0.54685701 -0.03049476 ...  0.13116697  0.11160576\n",
            "   0.0389133 ]\n",
            " [ 0.5729911  -0.42551609 -0.09453672 ...  0.03027458  0.05732893\n",
            "   0.01103053]\n",
            " [ 0.701764   -0.51350498  0.29390996 ... -0.02784028  0.01873501\n",
            "  -0.06648261]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "(178, 13)\n",
            "(178, 3)\n",
            "Train on 142 samples, validate on 36 samples\n",
            "Epoch 1/50\n",
            "142/142 [==============================] - 0s 3ms/sample - loss: 0.9559 - accuracy: 0.8239 - val_loss: 1.3991 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "142/142 [==============================] - 0s 504us/sample - loss: 0.5215 - accuracy: 0.8662 - val_loss: 1.5655 - val_accuracy: 0.0000e+00\n",
            "Epoch 3/50\n",
            "142/142 [==============================] - 0s 419us/sample - loss: 0.3258 - accuracy: 0.8944 - val_loss: 2.4834 - val_accuracy: 0.0000e+00\n",
            "Epoch 4/50\n",
            "142/142 [==============================] - 0s 404us/sample - loss: 0.3177 - accuracy: 0.8732 - val_loss: 2.4914 - val_accuracy: 0.0000e+00\n",
            "Epoch 5/50\n",
            "142/142 [==============================] - 0s 456us/sample - loss: 0.2457 - accuracy: 0.9014 - val_loss: 1.7540 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "142/142 [==============================] - 0s 556us/sample - loss: 0.2228 - accuracy: 0.9155 - val_loss: 1.4972 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "142/142 [==============================] - 0s 432us/sample - loss: 0.2186 - accuracy: 0.9085 - val_loss: 2.0745 - val_accuracy: 0.0000e+00\n",
            "Epoch 8/50\n",
            "142/142 [==============================] - 0s 438us/sample - loss: 0.1818 - accuracy: 0.9155 - val_loss: 1.8571 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "142/142 [==============================] - 0s 448us/sample - loss: 0.1719 - accuracy: 0.9155 - val_loss: 1.6223 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "142/142 [==============================] - 0s 507us/sample - loss: 0.2183 - accuracy: 0.9014 - val_loss: 1.3177 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "142/142 [==============================] - 0s 466us/sample - loss: 0.1698 - accuracy: 0.9085 - val_loss: 1.4210 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "142/142 [==============================] - 0s 464us/sample - loss: 0.1404 - accuracy: 0.9155 - val_loss: 1.0281 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "142/142 [==============================] - 0s 484us/sample - loss: 0.1458 - accuracy: 0.9155 - val_loss: 1.2199 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "142/142 [==============================] - 0s 439us/sample - loss: 0.1225 - accuracy: 0.9155 - val_loss: 1.1533 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "142/142 [==============================] - 0s 466us/sample - loss: 0.1166 - accuracy: 0.9155 - val_loss: 1.2292 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "142/142 [==============================] - 0s 438us/sample - loss: 0.1101 - accuracy: 0.9155 - val_loss: 1.4354 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "142/142 [==============================] - 0s 481us/sample - loss: 0.1145 - accuracy: 0.9155 - val_loss: 0.9413 - val_accuracy: 0.0000e+00\n",
            "Epoch 18/50\n",
            "142/142 [==============================] - 0s 473us/sample - loss: 0.1003 - accuracy: 0.9155 - val_loss: 1.2076 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "142/142 [==============================] - 0s 513us/sample - loss: 0.1147 - accuracy: 0.9085 - val_loss: 0.7585 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "142/142 [==============================] - 0s 456us/sample - loss: 0.1175 - accuracy: 0.9014 - val_loss: 0.8351 - val_accuracy: 0.0000e+00\n",
            "Epoch 21/50\n",
            "142/142 [==============================] - 0s 476us/sample - loss: 0.0858 - accuracy: 0.9155 - val_loss: 1.0616 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "142/142 [==============================] - 0s 448us/sample - loss: 0.0840 - accuracy: 0.9155 - val_loss: 0.8889 - val_accuracy: 0.0278\n",
            "Epoch 23/50\n",
            "142/142 [==============================] - 0s 457us/sample - loss: 0.0764 - accuracy: 0.9155 - val_loss: 0.9050 - val_accuracy: 0.0556\n",
            "Epoch 24/50\n",
            "142/142 [==============================] - 0s 459us/sample - loss: 0.0870 - accuracy: 0.9225 - val_loss: 0.7540 - val_accuracy: 0.6111\n",
            "Epoch 25/50\n",
            "142/142 [==============================] - 0s 421us/sample - loss: 0.0772 - accuracy: 0.9296 - val_loss: 0.7381 - val_accuracy: 0.6389\n",
            "Epoch 26/50\n",
            "142/142 [==============================] - 0s 455us/sample - loss: 0.0733 - accuracy: 0.9366 - val_loss: 0.8661 - val_accuracy: 0.3611\n",
            "Epoch 27/50\n",
            "142/142 [==============================] - 0s 506us/sample - loss: 0.0749 - accuracy: 0.9366 - val_loss: 0.7553 - val_accuracy: 0.6944\n",
            "Epoch 28/50\n",
            "142/142 [==============================] - 0s 474us/sample - loss: 0.0667 - accuracy: 0.9648 - val_loss: 1.0645 - val_accuracy: 0.3056\n",
            "Epoch 29/50\n",
            "142/142 [==============================] - 0s 435us/sample - loss: 0.0705 - accuracy: 0.9577 - val_loss: 0.7390 - val_accuracy: 0.8333\n",
            "Epoch 30/50\n",
            "142/142 [==============================] - 0s 460us/sample - loss: 0.0648 - accuracy: 0.9789 - val_loss: 0.6907 - val_accuracy: 0.9167\n",
            "Epoch 31/50\n",
            "142/142 [==============================] - 0s 477us/sample - loss: 0.0652 - accuracy: 0.9859 - val_loss: 0.7752 - val_accuracy: 0.7778\n",
            "Epoch 32/50\n",
            "142/142 [==============================] - 0s 446us/sample - loss: 0.0669 - accuracy: 0.9789 - val_loss: 0.6125 - val_accuracy: 1.0000\n",
            "Epoch 33/50\n",
            "142/142 [==============================] - 0s 482us/sample - loss: 0.0598 - accuracy: 0.9859 - val_loss: 0.5998 - val_accuracy: 0.9722\n",
            "Epoch 34/50\n",
            "142/142 [==============================] - 0s 471us/sample - loss: 0.0562 - accuracy: 0.9930 - val_loss: 0.6382 - val_accuracy: 0.8611\n",
            "Epoch 35/50\n",
            "142/142 [==============================] - 0s 482us/sample - loss: 0.0620 - accuracy: 0.9789 - val_loss: 0.5812 - val_accuracy: 0.9722\n",
            "Epoch 36/50\n",
            "142/142 [==============================] - 0s 438us/sample - loss: 0.0541 - accuracy: 0.9930 - val_loss: 0.5880 - val_accuracy: 0.9444\n",
            "Epoch 37/50\n",
            "142/142 [==============================] - 0s 432us/sample - loss: 0.0532 - accuracy: 0.9930 - val_loss: 0.5646 - val_accuracy: 0.9722\n",
            "Epoch 38/50\n",
            "142/142 [==============================] - 0s 501us/sample - loss: 0.0526 - accuracy: 0.9930 - val_loss: 0.5527 - val_accuracy: 0.9722\n",
            "Epoch 39/50\n",
            "142/142 [==============================] - 0s 423us/sample - loss: 0.0517 - accuracy: 0.9930 - val_loss: 0.5352 - val_accuracy: 1.0000\n",
            "Epoch 40/50\n",
            "142/142 [==============================] - 0s 421us/sample - loss: 0.2592 - accuracy: 0.9577 - val_loss: 0.5217 - val_accuracy: 1.0000\n",
            "Epoch 41/50\n",
            "142/142 [==============================] - 0s 507us/sample - loss: 0.0524 - accuracy: 0.9930 - val_loss: 0.5461 - val_accuracy: 0.9722\n",
            "Epoch 42/50\n",
            "142/142 [==============================] - 0s 443us/sample - loss: 0.0514 - accuracy: 0.9930 - val_loss: 0.5290 - val_accuracy: 1.0000\n",
            "Epoch 43/50\n",
            "142/142 [==============================] - 0s 435us/sample - loss: 0.0500 - accuracy: 0.9930 - val_loss: 0.5132 - val_accuracy: 1.0000\n",
            "Epoch 44/50\n",
            "142/142 [==============================] - 0s 456us/sample - loss: 0.0491 - accuracy: 0.9930 - val_loss: 0.5051 - val_accuracy: 1.0000\n",
            "Epoch 45/50\n",
            "142/142 [==============================] - 0s 497us/sample - loss: 0.0485 - accuracy: 0.9930 - val_loss: 0.5055 - val_accuracy: 1.0000\n",
            "Epoch 46/50\n",
            "142/142 [==============================] - 0s 466us/sample - loss: 0.0483 - accuracy: 0.9930 - val_loss: 0.4878 - val_accuracy: 1.0000\n",
            "Epoch 47/50\n",
            "142/142 [==============================] - 0s 518us/sample - loss: 0.0474 - accuracy: 0.9930 - val_loss: 0.4815 - val_accuracy: 1.0000\n",
            "Epoch 48/50\n",
            "142/142 [==============================] - 0s 447us/sample - loss: 0.0470 - accuracy: 0.9930 - val_loss: 0.4691 - val_accuracy: 1.0000\n",
            "Epoch 49/50\n",
            "142/142 [==============================] - 0s 445us/sample - loss: 0.0454 - accuracy: 0.9930 - val_loss: 0.4725 - val_accuracy: 0.9722\n",
            "Epoch 50/50\n",
            "142/142 [==============================] - 0s 481us/sample - loss: 0.0455 - accuracy: 0.9930 - val_loss: 0.4620 - val_accuracy: 0.9722\n",
            "[[9.99999821e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99991179e-01 5.96046448e-08 0.00000000e+00]\n",
            " [9.99997377e-01 1.19209290e-07 0.00000000e+00]\n",
            " [9.99999881e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99851584e-01 5.78165054e-06 0.00000000e+00]\n",
            " [9.99999881e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99999940e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99998927e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99999821e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99996424e-01 2.38418579e-07 0.00000000e+00]\n",
            " [9.99999642e-01 2.98023224e-08 0.00000000e+00]\n",
            " [9.99995232e-01 6.25848770e-07 4.17232513e-07]\n",
            " [9.99998987e-01 5.96046448e-08 0.00000000e+00]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [1.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
            " [9.99999285e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99998808e-01 1.19209290e-07 0.00000000e+00]\n",
            " [9.99984980e-01 1.10268593e-06 8.94069672e-08]\n",
            " [9.99999940e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99998689e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99997020e-01 2.98023224e-08 0.00000000e+00]\n",
            " [9.98900771e-01 4.61637974e-05 1.49011612e-07]\n",
            " [9.99997973e-01 5.96046448e-08 0.00000000e+00]\n",
            " [9.99072015e-01 1.14440918e-05 0.00000000e+00]\n",
            " [9.99788344e-01 1.87754631e-06 0.00000000e+00]\n",
            " [8.02232742e-01 5.24669886e-04 0.00000000e+00]\n",
            " [9.99996901e-01 8.94069672e-08 0.00000000e+00]\n",
            " [9.99339700e-01 6.47008419e-05 1.54972076e-06]\n",
            " [9.99999225e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99997497e-01 1.78813934e-07 0.00000000e+00]\n",
            " [9.99990880e-01 5.66244125e-07 5.96046448e-08]\n",
            " [9.99972522e-01 4.50015068e-06 2.05636024e-06]\n",
            " [9.99985099e-01 3.87430191e-07 0.00000000e+00]\n",
            " [9.99970019e-01 3.27825546e-07 0.00000000e+00]\n",
            " [9.99988914e-01 7.45058060e-07 2.98023224e-08]\n",
            " [9.99803841e-01 9.47713852e-06 0.00000000e+00]\n",
            " [9.99998271e-01 8.94069672e-08 0.00000000e+00]\n",
            " [9.99935210e-01 5.63263893e-06 2.08616257e-07]\n",
            " [9.73928511e-01 3.32087278e-04 0.00000000e+00]\n",
            " [9.99999702e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99964833e-01 1.31130219e-06 0.00000000e+00]\n",
            " [9.98164296e-01 2.54541636e-04 1.21593475e-05]\n",
            " [9.99999881e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.78172660e-01 1.62854791e-03 3.57627869e-06]\n",
            " [9.90623832e-01 3.16947699e-04 5.96046448e-08]\n",
            " [9.99975920e-01 2.20537186e-06 1.78813934e-07]\n",
            " [9.99999642e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99991655e-01 3.27825546e-07 0.00000000e+00]\n",
            " [9.99992132e-01 1.37090683e-06 1.31130219e-06]\n",
            " [9.99991536e-01 1.28149986e-06 9.23871994e-07]\n",
            " [9.99997139e-01 2.98023224e-07 1.19209290e-07]\n",
            " [9.99999821e-01 0.00000000e+00 0.00000000e+00]\n",
            " [9.99991059e-01 1.78813934e-07 0.00000000e+00]\n",
            " [9.99999404e-01 2.98023224e-08 0.00000000e+00]\n",
            " [9.99973297e-01 3.33786011e-06 5.06639481e-07]\n",
            " [9.99577641e-01 1.33275986e-04 1.77711248e-04]\n",
            " [9.99997973e-01 8.94069672e-08 0.00000000e+00]\n",
            " [9.99998450e-01 5.96046448e-08 0.00000000e+00]\n",
            " [9.99998093e-01 1.78813934e-07 0.00000000e+00]\n",
            " [5.96046448e-08 9.99717474e-01 0.00000000e+00]\n",
            " [5.74588776e-05 9.92238522e-01 2.98023224e-08]\n",
            " [9.39130783e-04 9.40894246e-01 3.00779045e-02]\n",
            " [1.59740448e-04 9.82879877e-01 1.49011612e-07]\n",
            " [4.47034836e-07 9.99258995e-01 0.00000000e+00]\n",
            " [7.15255737e-06 9.97052312e-01 0.00000000e+00]\n",
            " [1.92415714e-03 9.31633353e-01 3.36766243e-06]\n",
            " [1.32620335e-05 9.94555593e-01 0.00000000e+00]\n",
            " [1.49011612e-06 9.98997748e-01 0.00000000e+00]\n",
            " [6.28441572e-04 9.40649331e-01 1.78813934e-07]\n",
            " [3.54945660e-05 9.91392732e-01 0.00000000e+00]\n",
            " [8.91327858e-04 9.55411434e-01 3.34441662e-04]\n",
            " [9.59634781e-06 9.92376804e-01 0.00000000e+00]\n",
            " [4.40806150e-04 9.78223205e-01 1.13844872e-05]\n",
            " [4.50015068e-06 9.95004416e-01 0.00000000e+00]\n",
            " [3.96370888e-06 9.97570634e-01 0.00000000e+00]\n",
            " [6.55353069e-05 9.92973447e-01 2.98023224e-08]\n",
            " [3.38852406e-05 9.95241165e-01 5.96046448e-08]\n",
            " [4.00304794e-04 9.64853764e-01 1.19209290e-07]\n",
            " [1.29729509e-04 9.78795886e-01 0.00000000e+00]\n",
            " [9.03010368e-06 9.92883563e-01 0.00000000e+00]\n",
            " [0.00000000e+00 9.99930620e-01 0.00000000e+00]\n",
            " [2.44259834e-04 9.72001314e-01 2.98023224e-08]\n",
            " [2.08616257e-07 9.99544978e-01 0.00000000e+00]\n",
            " [5.18965721e-03 8.27213883e-01 7.11646914e-01]\n",
            " [5.99622726e-05 9.92398739e-01 2.98023224e-08]\n",
            " [5.75184822e-06 9.95778203e-01 0.00000000e+00]\n",
            " [1.72853470e-06 9.98789907e-01 0.00000000e+00]\n",
            " [2.98023224e-08 9.99793172e-01 0.00000000e+00]\n",
            " [3.57627869e-06 9.97551501e-01 0.00000000e+00]\n",
            " [2.98023224e-08 9.99784350e-01 0.00000000e+00]\n",
            " [1.02519989e-05 9.97395873e-01 0.00000000e+00]\n",
            " [7.42077827e-06 9.97758448e-01 0.00000000e+00]\n",
            " [3.36170197e-05 9.93672490e-01 1.84774399e-06]\n",
            " [8.64267349e-07 9.98698235e-01 0.00000000e+00]\n",
            " [4.47034836e-07 9.99280572e-01 0.00000000e+00]\n",
            " [4.24385071e-05 9.80007887e-01 0.00000000e+00]\n",
            " [5.06132841e-04 9.69353676e-01 9.65595245e-06]\n",
            " [7.74860382e-07 9.98789370e-01 0.00000000e+00]\n",
            " [5.69224358e-06 9.95691121e-01 0.00000000e+00]\n",
            " [4.47034836e-07 9.98487473e-01 0.00000000e+00]\n",
            " [8.52346420e-06 9.95618343e-01 0.00000000e+00]\n",
            " [8.52346420e-05 9.91252184e-01 8.94069672e-08]\n",
            " [1.60038471e-05 9.94389296e-01 0.00000000e+00]\n",
            " [1.01327896e-06 9.99196589e-01 0.00000000e+00]\n",
            " [3.75509262e-06 9.97295022e-01 0.00000000e+00]\n",
            " [2.68220901e-06 9.97830749e-01 0.00000000e+00]\n",
            " [1.24275684e-05 9.96286690e-01 0.00000000e+00]\n",
            " [1.30265951e-04 9.89067674e-01 3.57627869e-07]\n",
            " [1.06096268e-05 9.97191370e-01 0.00000000e+00]\n",
            " [1.60932541e-06 9.97128725e-01 0.00000000e+00]\n",
            " [1.80065632e-04 9.80724990e-01 6.25848770e-07]\n",
            " [3.12924385e-06 9.98395920e-01 0.00000000e+00]\n",
            " [1.96993351e-05 9.93465185e-01 0.00000000e+00]\n",
            " [1.69873238e-06 9.99028563e-01 0.00000000e+00]\n",
            " [5.06639481e-07 9.99337256e-01 0.00000000e+00]\n",
            " [0.00000000e+00 9.99989510e-01 0.00000000e+00]\n",
            " [2.68220901e-07 9.99302387e-01 0.00000000e+00]\n",
            " [1.08480453e-05 9.95885611e-01 0.00000000e+00]\n",
            " [1.30662322e-03 9.52226639e-01 8.40693712e-04]\n",
            " [6.19888306e-06 9.95830417e-01 0.00000000e+00]\n",
            " [2.71201134e-06 9.97263432e-01 0.00000000e+00]\n",
            " [0.00000000e+00 9.99778628e-01 0.00000000e+00]\n",
            " [6.25848770e-06 9.94880438e-01 0.00000000e+00]\n",
            " [2.45690346e-04 9.72867966e-01 8.94069672e-08]\n",
            " [4.91738319e-06 9.95819330e-01 0.00000000e+00]\n",
            " [5.36441803e-07 9.99113858e-01 0.00000000e+00]\n",
            " [8.33272934e-05 9.90865469e-01 2.08616257e-07]\n",
            " [5.66244125e-07 9.99228656e-01 0.00000000e+00]\n",
            " [3.01003456e-06 9.98524904e-01 0.00000000e+00]\n",
            " [1.32530928e-04 9.84119773e-01 1.49011612e-07]\n",
            " [7.69436359e-03 5.51516533e-01 9.98813629e-01]\n",
            " [1.61288083e-02 5.38906395e-01 9.96580541e-01]\n",
            " [9.91082191e-03 5.47221780e-01 9.98297274e-01]\n",
            " [1.40436590e-02 5.41278183e-01 9.97196317e-01]\n",
            " [4.79742885e-03 5.59489608e-01 9.99394774e-01]\n",
            " [1.23956800e-02 5.43411076e-01 9.97655272e-01]\n",
            " [2.11176276e-02 5.34268022e-01 9.94962335e-01]\n",
            " [3.07347178e-02 5.27751982e-01 9.91337180e-01]\n",
            " [1.87168419e-02 5.36348820e-01 9.95765567e-01]\n",
            " [3.09061408e-02 5.27654767e-01 9.91267204e-01]\n",
            " [3.43146920e-02 5.25823355e-01 9.89835620e-01]\n",
            " [3.85594368e-02 5.23772895e-01 9.87957001e-01]\n",
            " [3.70267630e-02 5.24487138e-01 9.88647342e-01]\n",
            " [5.14219403e-02 5.57664871e-01 9.69302118e-01]\n",
            " [2.88043022e-02 5.28883755e-01 9.92114067e-01]\n",
            " [2.39603817e-02 5.32083392e-01 9.93956447e-01]\n",
            " [2.63773799e-02 5.30415595e-01 9.93056655e-01]\n",
            " [3.80271971e-02 5.24017811e-01 9.88198280e-01]\n",
            " [3.15317512e-02 5.27304530e-01 9.91009712e-01]\n",
            " [1.44449174e-02 5.40796101e-01 9.97080803e-01]\n",
            " [6.26108050e-03 5.55002093e-01 9.99115705e-01]\n",
            " [5.90273738e-03 5.55996597e-01 9.99186873e-01]\n",
            " [3.19379568e-03 5.66317081e-01 9.99660552e-01]\n",
            " [2.05670893e-02 5.34724057e-01 9.95150328e-01]\n",
            " [7.63142109e-03 5.51655710e-01 9.98827457e-01]\n",
            " [3.42825949e-02 5.25839806e-01 9.89849567e-01]\n",
            " [3.08200419e-02 5.27703464e-01 9.91302252e-01]\n",
            " [2.12332904e-02 7.29210734e-01 5.54951072e-01]\n",
            " [1.81373656e-02 5.36890090e-01 9.95952666e-01]\n",
            " [1.13233626e-02 5.44954062e-01 9.97940063e-01]\n",
            " [1.79016292e-02 5.37115097e-01 9.96028066e-01]\n",
            " [5.28822839e-02 5.18163383e-01 9.80891168e-01]\n",
            " [3.15150023e-02 5.27313769e-01 9.91016567e-01]\n",
            " [3.14487219e-02 5.27350605e-01 9.91043925e-01]\n",
            " [3.38922143e-02 5.26040614e-01 9.90016818e-01]\n",
            " [2.10281610e-02 5.34341335e-01 9.94993150e-01]\n",
            " [1.47617549e-01 3.47685754e-01 9.30850029e-01]\n",
            " [3.02743018e-02 5.28015554e-01 9.91524696e-01]\n",
            " [5.53111136e-02 5.17358124e-01 9.79588687e-01]\n",
            " [3.47765088e-02 5.24028122e-01 9.89606261e-01]\n",
            " [1.72548294e-02 5.37747681e-01 9.96232629e-01]\n",
            " [1.55219436e-02 5.39564192e-01 9.96763587e-01]\n",
            " [8.04580450e-02 4.65083361e-01 9.61738348e-01]\n",
            " [4.68941063e-01 1.22818977e-01 7.82604516e-01]\n",
            " [5.15011549e-02 5.18636942e-01 9.81618762e-01]\n",
            " [2.99144387e-02 5.28224230e-01 9.91670132e-01]\n",
            " [3.12834531e-02 5.27442753e-01 9.91112173e-01]\n",
            " [1.13134459e-01 3.97268981e-01 9.44170892e-01]]\n",
            "(178, 3)\n",
            "Model: \"model_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_14 (InputLayer)        [(None, 13)]              0         \n",
            "_________________________________________________________________\n",
            "dense_66 (Dense)             (None, 10)                140       \n",
            "_________________________________________________________________\n",
            "dense_67 (Dense)             (None, 400)               4400      \n",
            "_________________________________________________________________\n",
            "dense_68 (Dense)             (None, 800)               320800    \n",
            "_________________________________________________________________\n",
            "dense_69 (Dense)             (None, 400)               320400    \n",
            "_________________________________________________________________\n",
            "dense_70 (Dense)             (None, 10)                4010      \n",
            "_________________________________________________________________\n",
            "dense_71 (Dense)             (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 649,783\n",
            "Trainable params: 649,783\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}