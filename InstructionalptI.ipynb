{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "InstructionalptI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMQwssS8tfgSrGN1wEyzj3u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lordfiftyfive/Seattle-Tensorflow/blob/master/InstructionalptI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "91CKWv2HO24E",
        "colab_type": "text"
      },
      "source": [
        "If you do NOT have the following libraries installed on your local machine then you will want to:\n",
        " 1. install anaconda navigator and install the spyder enviroment withen anaconda navigator \n",
        " 2. open up a terminal withen anaconda navigator and use conda install (name of library)\n",
        "\n",
        "\n",
        "\n",
        " \n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFvCEJw9MlPl",
        "colab_type": "code",
        "outputId": "278bec05-ca3e-4a4a-8d60-b5f7cda13208",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        }
      },
      "source": [
        "!pip install quandl\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting quandl\n",
            "  Downloading https://files.pythonhosted.org/packages/07/ab/8cd479fba8a9b197a43a0d55dd534b066fb8e5a0a04b5c0384cbc5d663aa/Quandl-3.5.0-py2.py3-none-any.whl\n",
            "Collecting inflection>=0.3.1\n",
            "  Downloading https://files.pythonhosted.org/packages/d5/35/a6eb45b4e2356fe688b21570864d4aa0d0a880ce387defe9c589112077f8/inflection-0.3.1.tar.gz\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.6/dist-packages (from quandl) (8.0.2)\n",
            "Requirement already satisfied: pandas>=0.14 in /usr/local/lib/python3.6/dist-packages (from quandl) (0.25.3)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.6/dist-packages (from quandl) (2.6.1)\n",
            "Requirement already satisfied: numpy>=1.8 in /usr/local/lib/python3.6/dist-packages (from quandl) (1.17.5)\n",
            "Requirement already satisfied: requests>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from quandl) (2.21.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from quandl) (1.12.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.14->quandl) (2018.9)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2019.11.28)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (3.0.4)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.7.0->quandl) (2.8)\n",
            "Building wheels for collected packages: inflection\n",
            "  Building wheel for inflection (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for inflection: filename=inflection-0.3.1-cp36-none-any.whl size=6076 sha256=5acd675758d0c135d4489884d8a94d4c7e34029acdab1782fbdd9439267dbe8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/5a/d3/6fc3bf6516d2a3eb7e18f9f28b472110b59325f3f258fe9211\n",
            "Successfully built inflection\n",
            "Installing collected packages: inflection, quandl\n",
            "Successfully installed inflection-0.3.1 quandl-3.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tfNwAr7UNjHz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import numpy as np\n",
        "import quandl\n",
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYoIdS-gKi3v",
        "colab_type": "text"
      },
      "source": [
        "    First we are going to want to import our data here we are going to show how\n",
        "     to import using one of the most common data format which is the csv file "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YAAC80oFgrvn",
        "colab_type": "code",
        "outputId": "117c4255-762e-4be8-d3f0-925e27bab2d6",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": "OK"
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e2c77bc0-7fbd-4ac3-ad7e-48f0aa45c6aa\" name=\"files[]\" multiple disabled />\n",
              "     <output id=\"result-e2c77bc0-7fbd-4ac3-ad7e-48f0aa45c6aa\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving 930-data-export.csv to 930-data-export (1).csv\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'930-data-export.csv': b'\\xef\\xbb\\xbf\"Region Code\",\"Timestamp (Hour Ending)\",\"Demand (MWh)\",\"Demand Forecast (MWh)\",\"Net Generation (MWh)\",\"Total Interchange (MWh)\"\\nUS48,\"1/13/2020 12 a.m. EST\",412290,407010,392570,2977\\nUS48,\"1/13/2020 1 a.m. EST\",396187,389484,384466,3557\\nUS48,\"1/13/2020 2 a.m. EST\",383358,378106,373763,3755\\nUS48,\"1/13/2020 3 a.m. EST\",376657,371685,368427,3617\\nUS48,\"1/13/2020 4 a.m. EST\",373911,371433,365626,3968\\nUS48,\"1/13/2020 5 a.m. EST\",377435,379238,372388,3885\\nUS48,\"1/13/2020 6 a.m. EST\",391321,398288,390232,3724\\nUS48,\"1/13/2020 7 a.m. EST\",420939,426194,419251,4251\\nUS48,\"1/13/2020 8 a.m. EST\",451411,449840,445065,4899\\nUS48,\"1/13/2020 9 a.m. EST\",466590,462151,458984,3777\\nUS48,\"1/13/2020 10 a.m. EST\",474128,466698,463784,2587\\nUS48,\"1/13/2020 11 a.m. EST\",476935,467473,465528,2413\\nUS48,\"1/13/2020 12 p.m. EST\",473527,463802,464555,3258\\nUS48,\"1/13/2020 1 p.m. EST\",469042,458334,459451,3658\\nUS48,\"1/13/2020 2 p.m. EST\",463807,454026,454014,3540\\nUS48,\"1/13/2020 3 p.m. EST\",461050,450478,450718,2629\\nUS48,\"1/13/2020 4 p.m. EST\",482699,449834,449548,2667\\nUS48,\"1/13/2020 5 p.m. EST\",457355,456182,454486,3712\\nUS48,\"1/13/2020 6 p.m. EST\",466943,464312,461205,4701\\nUS48,\"1/13/2020 7 p.m. EST\",479553,473890,469744,4811\\nUS48,\"1/13/2020 8 p.m. EST\",483558,476427,473429,2882\\nUS48,\"1/13/2020 9 p.m. EST\",479106,472856,465850,4294\\nUS48,\"1/13/2020 10 p.m. EST\",465532,458270,449632,3168\\nUS48,\"1/13/2020 11 p.m. EST\",468760,437842,432086,2516\\nUS48,\"1/14/2020 12 a.m. EST\",426254,416012,405418,2353\\nUS48,\"1/14/2020 1 a.m. EST\",402196,397248,390650,2659\\nUS48,\"1/14/2020 2 a.m. EST\",393223,384741,380533,1929\\nUS48,\"1/14/2020 3 a.m. EST\",382796,376001,372151,2638\\nUS48,\"1/14/2020 4 a.m. EST\",378596,373408,368678,1984\\nUS48,\"1/14/2020 5 a.m. EST\",379293,379332,372224,2172\\nUS48,\"1/14/2020 6 a.m. EST\",389607,396075,384969,1834\\nUS48,\"1/14/2020 7 a.m. EST\",413958,421503,410263,2027\\nUS48,\"1/14/2020 8 a.m. EST\",439522,443869,431831,3126\\nUS48,\"1/14/2020 9 a.m. EST\",451986,454713,444514,3983\\nUS48,\"1/14/2020 10 a.m. EST\",459440,460033,449327,3426\\nUS48,\"1/14/2020 11 a.m. EST\",462488,461730,451464,3146\\nUS48,\"1/14/2020 12 p.m. EST\",460619,458973,450228,3225\\nUS48,\"1/14/2020 1 p.m. EST\",457269,455698,447608,3399\\nUS48,\"1/14/2020 2 p.m. EST\",454194,453132,444387,3722\\nUS48,\"1/14/2020 3 p.m. EST\",451344,451396,441137,2448\\nUS48,\"1/14/2020 4 p.m. EST\",475519,451950,441885,2692\\nUS48,\"1/14/2020 5 p.m. EST\",430021,457626,445378,2934\\nUS48,\"1/14/2020 6 p.m. EST\",460054,465145,452756,3774\\nUS48,\"1/14/2020 7 p.m. EST\",470792,473445,457782,3160\\nUS48,\"1/14/2020 8 p.m. EST\",474856,475349,459931,1584\\nUS48,\"1/14/2020 9 p.m. EST\",472403,471739,455844,1124\\nUS48,\"1/14/2020 10 p.m. EST\",460373,457830,442975,1406\\nUS48,\"1/14/2020 11 p.m. EST\",464057,437439,424745,506\\nUS48,\"1/15/2020 12 a.m. EST\",423781,415531,408584,1553\\nUS48,\"1/15/2020 1 a.m. EST\",404545,398414,391260,2385\\nUS48,\"1/15/2020 2 a.m. EST\",390897,383257,379225,3133\\nUS48,\"1/15/2020 3 a.m. EST\",381912,374153,370792,2438\\nUS48,\"1/15/2020 4 a.m. EST\",376392,371155,367256,2993\\nUS48,\"1/15/2020 5 a.m. EST\",377778,376498,370556,2357\\nUS48,\"1/15/2020 6 a.m. EST\",386077,393515,382691,1650\\nUS48,\"1/15/2020 7 a.m. EST\",411111,418421,405967,1977\\nUS48,\"1/15/2020 8 a.m. EST\",435987,440003,425928,1769\\nUS48,\"1/15/2020 9 a.m. EST\",447084,450958,436535,2195\\nUS48,\"1/15/2020 10 a.m. EST\",452808,456744,439517,1282\\nUS48,\"1/15/2020 11 a.m. EST\",453618,458837,440404,1511\\nUS48,\"1/15/2020 12 p.m. EST\",452786,456530,440169,1098\\nUS48,\"1/15/2020 1 p.m. EST\",449322,453996,437929,2121\\nUS48,\"1/15/2020 2 p.m. EST\",446829,451406,436209,2677\\nUS48,\"1/15/2020 3 p.m. EST\",445870,450391,436247,2975\\nUS48,\"1/15/2020 4 p.m. EST\",446390,451163,438899,3178\\nUS48,\"1/15/2020 5 p.m. EST\",450168,457255,445729,3362\\nUS48,\"1/15/2020 6 p.m. EST\",459824,464361,453110,3740\\nUS48,\"1/15/2020 7 p.m. EST\",469828,472660,457287,3146\\nUS48,\"1/15/2020 8 p.m. EST\",473300,474951,460937,3711\\nUS48,\"1/15/2020 9 p.m. EST\",469198,470495,454558,3225\\nUS48,\"1/15/2020 10 p.m. EST\",458489,456152,442481,3191\\nUS48,\"1/15/2020 11 p.m. EST\",441342,436164,424819,2232\\nUS48,\"1/16/2020 12 a.m. EST\",421109,414336,407062,1957\\nUS48,\"1/16/2020 1 a.m. EST\",402523,396276,387895,2227\\nUS48,\"1/16/2020 2 a.m. EST\",389979,383396,376707,2857\\nUS48,\"1/16/2020 3 a.m. EST\",381205,374526,368137,2818\\nUS48,\"1/16/2020 4 a.m. EST\",376492,372192,366204,3257\\nUS48,\"1/16/2020 5 a.m. EST\",378562,378398,370227,3585\\nUS48,\"1/16/2020 6 a.m. EST\",388591,395500,384456,3633\\nUS48,\"1/16/2020 7 a.m. EST\",415832,422221,409464,4049\\nUS48,\"1/16/2020 8 a.m. EST\",444906,446143,431596,4385\\nUS48,\"1/16/2020 9 a.m. EST\",460077,459092,442993,3918\\nUS48,\"1/16/2020 10 a.m. EST\",467868,465944,450332,3690\\nUS48,\"1/16/2020 11 a.m. EST\",470695,468643,453210,4610\\nUS48,\"1/16/2020 12 p.m. EST\",469098,467089,456565,3935\\nUS48,\"1/16/2020 1 p.m. EST\",466975,463873,447874,4067\\nUS48,\"1/16/2020 2 p.m. EST\",465834,461117,451952,4409\\nUS48,\"1/16/2020 3 p.m. EST\",464640,459333,449061,4014\\nUS48,\"1/16/2020 4 p.m. EST\",463291,460113,450851,4575\\nUS48,\"1/16/2020 5 p.m. EST\",467242,465923,458895,4375\\nUS48,\"1/16/2020 6 p.m. EST\",478388,474473,467337,4650\\nUS48,\"1/16/2020 7 p.m. EST\",494333,485605,478102,4243\\nUS48,\"1/16/2020 8 p.m. EST\",500163,490752,480858,3976\\nUS48,\"1/16/2020 9 p.m. EST\",500130,489351,477930,3496\\nUS48,\"1/16/2020 10 p.m. EST\",489860,477750,467695,3543\\nUS48,\"1/16/2020 11 p.m. EST\",471619,459509,452385,3449\\nUS48,\"1/17/2020 12 a.m. EST\",452181,439536,423538,3335\\nUS48,\"1/17/2020 1 a.m. EST\",433846,422052,418267,3426\\nUS48,\"1/17/2020 2 a.m. EST\",421202,380746,414561,5184\\nUS48,\"1/17/2020 3 a.m. EST\",413935,375901,405237,4991\\nUS48,\"1/17/2020 4 a.m. EST\",411358,366957,403435,4380\\nUS48,\"1/17/2020 5 a.m. EST\",413997,374773,408271,4780\\nUS48,\"1/17/2020 6 a.m. EST\",426062,393458,425060,5340\\nUS48,\"1/17/2020 7 a.m. EST\",453110,419712,451322,6048\\nUS48,\"1/17/2020 8 a.m. EST\",479239,439736,473428,6235\\nUS48,\"1/17/2020 9 a.m. EST\",490382,447931,481274,5779\\nUS48,\"1/17/2020 10 a.m. EST\",493854,449424,483557,5204\\nUS48,\"1/17/2020 11 a.m. EST\",493711,447301,483216,4587\\nUS48,\"1/17/2020 12 p.m. EST\",487661,440762,479145,5065\\nUS48,\"1/17/2020 1 p.m. EST\",480618,433545,471638,4494\\nUS48,\"1/17/2020 2 p.m. EST\",473928,427174,466370,4044\\nUS48,\"1/17/2020 3 p.m. EST\",469032,423021,464051,4420\\nUS48,\"1/17/2020 4 p.m. EST\",466812,422537,462432,4718\\nUS48,\"1/17/2020 5 p.m. EST\",469378,428662,469764,4871\\nUS48,\"1/17/2020 6 p.m. EST\",480058,435637,477613,4928\\nUS48,\"1/17/2020 7 p.m. EST\",491120,443903,484842,5788\\nUS48,\"1/17/2020 8 p.m. EST\",493264,445551,484976,3933\\nUS48,\"1/17/2020 9 p.m. EST\",490031,442951,481725,3391\\nUS48,\"1/17/2020 10 p.m. EST\",480364,433526,469279,3414\\nUS48,\"1/17/2020 11 p.m. EST\",466193,419140,454255,3303\\nUS48,\"1/18/2020 12 a.m. EST\",449235,401839,434138,3199\\nUS48,\"1/18/2020 1 a.m. EST\",433455,388656,421878,2270\\nUS48,\"1/18/2020 2 a.m. EST\",420028,372626,409100,2355\\nUS48,\"1/18/2020 3 a.m. EST\",411290,365135,399490,1926\\nUS48,\"1/18/2020 4 a.m. EST\",403807,361397,392853,1840\\nUS48,\"1/18/2020 5 a.m. EST\",401604,362077,391554,1805\\nUS48,\"1/18/2020 6 a.m. EST\",403735,367773,395401,2049\\nUS48,\"1/18/2020 7 a.m. EST\",411079,378200,402849,2093\\nUS48,\"1/18/2020 8 a.m. EST\",424436,390290,417573,2553\\nUS48,\"1/18/2020 9 a.m. EST\",439066,402458,431115,3460\\nUS48,\"1/18/2020 10 a.m. EST\",451145,410937,443886,3098\\nUS48,\"1/18/2020 11 a.m. EST\",459660,413546,452103,3525\\nUS48,\"1/18/2020 12 p.m. EST\",461354,410803,454531,4647\\nUS48,\"1/18/2020 1 p.m. EST\",459199,405154,451092,5119\\nUS48,\"1/18/2020 2 p.m. EST\",454938,399495,447161,5827\\nUS48,\"1/18/2020 3 p.m. EST\",449758,395506,443544,5781\\nUS48,\"1/18/2020 4 p.m. EST\",446429,394950,440793,5971\\nUS48,\"1/18/2020 5 p.m. EST\",447368,398538,445400,6110\\nUS48,\"1/18/2020 6 p.m. EST\",473412,404974,448718,6100\\nUS48,\"1/18/2020 7 p.m. EST\",443344,412539,453812,5362\\nUS48,\"1/18/2020 8 p.m. EST\",464828,414127,454741,5014\\nUS48,\"1/18/2020 9 p.m. EST\",462589,412109,449293,3377\\nUS48,\"1/18/2020 10 p.m. EST\",453773,403453,441626,2857\\nUS48,\"1/18/2020 11 p.m. EST\",442659,390545,429944,2445\\nUS48,\"1/19/2020 12 a.m. EST\",448545,377311,424081,3058\\nUS48,\"1/19/2020 1 a.m. EST\",415392,366128,404352,3040\\nUS48,\"1/19/2020 2 a.m. EST\",406575,393819,395701,3024\\nUS48,\"1/19/2020 3 a.m. EST\",399181,387163,388859,3028\\nUS48,\"1/19/2020 4 a.m. EST\",395031,385568,386262,2497\\nUS48,\"1/19/2020 5 a.m. EST\",395321,387924,386332,2502\\nUS48,\"1/19/2020 6 a.m. EST\",397897,395113,390421,2386\\nUS48,\"1/19/2020 7 a.m. EST\",407317,406588,400497,2603\\nUS48,\"1/19/2020 8 a.m. EST\",420885,422149,414908,2442\\nUS48,\"1/19/2020 9 a.m. EST\",437903,437758,430369,2238\\nUS48,\"1/19/2020 10 a.m. EST\",450853,448011,441232,1938\\nUS48,\"1/19/2020 11 a.m. EST\",453333,449959,443947,2698\\nUS48,\"1/19/2020 12 p.m. EST\",450411,447103,441130,2790\\nUS48,\"1/19/2020 1 p.m. EST\",446021,442747,437446,3156\\nUS48,\"1/19/2020 2 p.m. EST\",441024,437561,432136,2950\\nUS48,\"1/19/2020 3 p.m. EST\",437243,433406,428784,3306\\nUS48,\"1/19/2020 4 p.m. EST\",435576,433123,428925,3396\\nUS48,\"1/19/2020 5 p.m. EST\",441111,440722,436943,3650\\nUS48,\"1/19/2020 6 p.m. EST\",457449,454606,449841,2367\\nUS48,\"1/19/2020 7 p.m. EST\",476716,472078,465921,2145\\nUS48,\"1/19/2020 8 p.m. EST\",487422,481477,475861,2044\\nUS48,\"1/19/2020 9 p.m. EST\",491662,483401,477096,921\\nUS48,\"1/19/2020 10 p.m. EST\",485619,477366,471310,1944\\nUS48,\"1/19/2020 11 p.m. EST\",475857,465917,461410,2389\\nUS48,\"1/20/2020 12 a.m. EST\",461223,451050,439557,2738\\nUS48,\"1/20/2020 1 a.m. EST\",449412,439423,,\\nUS48,\"1/20/2020 2 a.m. EST\",442192,433385,,\\nUS48,\"1/20/2020 3 a.m. EST\",438007,430647,,\\nUS48,\"1/20/2020 4 a.m. EST\",437428,434272,,\\nUS48,\"1/20/2020 5 a.m. EST\",442598,444052,,\\nUS48,\"1/20/2020 6 a.m. EST\",456569,462500,,\\nUS48,\"1/20/2020 7 a.m. EST\",480598,487170,,\\nUS48,\"1/20/2020 8 a.m. EST\",505034,509542,,\\nUS48,\"1/20/2020 9 a.m. EST\",520114,521154,,\\nUS48,\"1/20/2020 10 a.m. EST\",525757,521796,,\\nUS48,\"1/20/2020 11 a.m. EST\",523017,515462,,\\nUS48,\"1/20/2020 12 p.m. EST\",516270,506293,,\\nUS48,\"1/20/2020 1 p.m. EST\",506637,494807,,\\nUS48,\"1/20/2020 2 p.m. EST\",,485316,,\\nUS48,\"1/20/2020 3 p.m. EST\",,477848,,\\nUS48,\"1/20/2020 4 p.m. EST\",,475395,,\\nUS48,\"1/20/2020 5 p.m. EST\",,482383,,\\nUS48,\"1/20/2020 6 p.m. EST\",,496691,,\\nUS48,\"1/20/2020 7 p.m. EST\",,515522,,\\nUS48,\"1/20/2020 8 p.m. EST\",,525340,,\\nUS48,\"1/20/2020 9 p.m. EST\",,525704,,\\nUS48,\"1/20/2020 10 p.m. EST\",,515812,,\\nUS48,\"1/20/2020 11 p.m. EST\",,498895,,\\nUS48,\"1/21/2020 12 a.m. EST\",,481688,,\\n'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9PszKmVvgIbG",
        "colab_type": "code",
        "outputId": "611beadc-86c1-4227-d76c-e250c2d83e3a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 260
        }
      },
      "source": [
        " electricity = pd.read_csv('930-data-export.csv')\n",
        " print(electricity)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    Region Code  ... Total Interchange (MWh)\n",
            "0          US48  ...                  2977.0\n",
            "1          US48  ...                  3557.0\n",
            "2          US48  ...                  3755.0\n",
            "3          US48  ...                  3617.0\n",
            "4          US48  ...                  3968.0\n",
            "..          ...  ...                     ...\n",
            "188        US48  ...                     NaN\n",
            "189        US48  ...                     NaN\n",
            "190        US48  ...                     NaN\n",
            "191        US48  ...                     NaN\n",
            "192        US48  ...                     NaN\n",
            "\n",
            "[193 rows x 6 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RywEUlRkJ7W4",
        "colab_type": "text"
      },
      "source": [
        "https://www.eia.gov/beta/electricity/gridmonitor/dashboard/electric_overview/US48/US48?src=email"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qLvB2yxOML9P",
        "colab_type": "code",
        "outputId": "dc2f0536-2d50-45d0-c6ad-089b79c31b7e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 365
        }
      },
      "source": [
        "\n",
        "Data_to_predict = quandl.get(\"FRBP/GDPPLUS_042619\",  collapse=\"quarterly\")\n",
        "datafive = quandl.get(\"FRED/PCETRIM1M158SFRBDAL\",  collapse=\"quarterly\", start_date=\"1977-02-01\",end_date=\"2016-03-31\")#quandl.get(\"FRED/VALEXPUSM052N\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\", collapse=\"quarterly\", start_date=\"1960-09-30\")#quandl.get(\"WWDI/USA_NE_GDI_TOTL_CD\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", start_date=\"1970-12-31\")\n",
        "Data_To_predict = Data_to_predict.values\n",
        "#the below column reduces the dimension of the data set by 1. We need to do this sometimes in order to put it into a pandas dataframe which we can easily manipulate\n",
        "\n",
        "electricity= pd.DataFrame(electricity)\n",
        "datafive= pd.DataFrame(datafive)\n",
        "\n",
        "plt.plot(datafive)\n",
        "print(datafive)\n",
        "print(electricity)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LimitExceededError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLimitExceededError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-ffc4549e78d7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mData_to_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FRBP/GDPPLUS_042619\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcollapse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"quarterly\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdatafive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquandl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FRED/PCETRIM1M158SFRBDAL\"\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mcollapse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"quarterly\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1977-02-01\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mend_date\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"2016-03-31\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#quandl.get(\"FRED/VALEXPUSM052N\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", transform=\"rdiff\", collapse=\"quarterly\", start_date=\"1960-09-30\")#quandl.get(\"WWDI/USA_NE_GDI_TOTL_CD\", authtoken=\"DNMZo2iRzVENxpxqHBKF\", start_date=\"1970-12-31\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mData_To_predict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mData_to_predict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#the below column reduces the dimension of the data set by 1. We need to do this sometimes in order to put it into a pandas dataframe which we can easily manipulate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/get.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(dataset, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'column_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_args\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'code'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle_column_not_found\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m     \u001b[0;31m# Array\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/model/dataset.py\u001b[0m in \u001b[0;36mdata\u001b[0;34m(self, **options)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mupdated_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmerge_options\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mData\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mupdated_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mNotFoundError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhandle_not_found_error\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/operations/list.py\u001b[0m in \u001b[0;36mall\u001b[0;34m(cls, **options)\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstructed_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'params'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mConnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'get'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mresponse_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mUtil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_to_dates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mabs_url\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%s/%s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mApiConfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_base\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_verb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabs_url\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mclassmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mexecute_request\u001b[0;34m(cls, http_verb, url, **options)\u001b[0m\n\u001b[1;32m     48\u001b[0m                                        **options)\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                 \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_api_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/quandl/connection.py\u001b[0m in \u001b[0;36mhandle_api_error\u001b[0;34m(cls, resp)\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mklass\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0md_klass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode_letter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mQuandlError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mLimitExceededError\u001b[0m: (Status 429) (Quandl Error QELx01) You have exceeded the anonymous user limit of 50 calls per day. To make more calls today, please register for a free Quandl account and then include your API key with your requests."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TItxs-oC31ud",
        "colab_type": "code",
        "outputId": "0651c411-ac78-41e2-8e34-55a1f18836f3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Standardize the data\n",
        "\n",
        "#note on minmaxScalar vs standard scalar using standard scalar is generally inadvisable because of the fact that standard scalar tries to force the data to conform to a standard normal distribution. if the data does not conform to a\n",
        "#standard normal distribution this can end up distorting the data. \n",
        "Scalar = MinMaxScaler(feature_range=(0,1))\n",
        "\n",
        "b = Scalar.fit_transform(datafive)\n",
        "\n",
        "#b = np.log10(b)\n",
        "\n",
        "\n",
        "plt.plot(datafive)\n",
        "print(b)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.64537445]\n",
            " [0.6222467 ]\n",
            " [0.47907489]\n",
            " [0.54515419]\n",
            " [0.61123348]\n",
            " [0.64537445]\n",
            " [0.71696035]\n",
            " [0.62665198]\n",
            " [0.66079295]\n",
            " [0.70484581]\n",
            " [0.86894273]\n",
            " [0.8314978 ]\n",
            " [0.97907489]\n",
            " [0.91519824]\n",
            " [1.        ]\n",
            " [0.83259912]\n",
            " [0.73348018]\n",
            " [0.58810573]\n",
            " [0.74669604]\n",
            " [0.48898678]\n",
            " [0.49779736]\n",
            " [0.54405286]\n",
            " [0.27643172]\n",
            " [0.26321586]\n",
            " [0.18832599]\n",
            " [0.30837004]\n",
            " [0.39757709]\n",
            " [0.20594714]\n",
            " [0.38325991]\n",
            " [0.31828194]\n",
            " [0.3777533 ]\n",
            " [0.35132159]\n",
            " [0.37885463]\n",
            " [0.39757709]\n",
            " [0.32709251]\n",
            " [0.36123348]\n",
            " [0.12555066]\n",
            " [0.37334802]\n",
            " [0.34911894]\n",
            " [0.2753304 ]\n",
            " [0.37555066]\n",
            " [0.29515419]\n",
            " [0.31277533]\n",
            " [0.14647577]\n",
            " [0.35462555]\n",
            " [0.43281938]\n",
            " [0.52312775]\n",
            " [0.33590308]\n",
            " [0.36784141]\n",
            " [0.28193833]\n",
            " [0.31497797]\n",
            " [0.3623348 ]\n",
            " [0.53744493]\n",
            " [0.46475771]\n",
            " [0.30396476]\n",
            " [0.30726872]\n",
            " [0.15198238]\n",
            " [0.17070485]\n",
            " [0.28634361]\n",
            " [0.34581498]\n",
            " [0.26651982]\n",
            " [0.17290749]\n",
            " [0.10022026]\n",
            " [0.20704846]\n",
            " [0.18502203]\n",
            " [0.12555066]\n",
            " [0.16079295]\n",
            " [0.17180617]\n",
            " [0.26101322]\n",
            " [0.21475771]\n",
            " [0.1277533 ]\n",
            " [0.12555066]\n",
            " [0.20704846]\n",
            " [0.1123348 ]\n",
            " [0.14647577]\n",
            " [0.17621145]\n",
            " [0.21475771]\n",
            " [0.16079295]\n",
            " [0.22246696]\n",
            " [0.16189427]\n",
            " [0.14757709]\n",
            " [0.15418502]\n",
            " [0.20374449]\n",
            " [0.06057269]\n",
            " [0.20594714]\n",
            " [0.06718062]\n",
            " [0.13546256]\n",
            " [0.10242291]\n",
            " [0.10462555]\n",
            " [0.09361233]\n",
            " [0.1938326 ]\n",
            " [0.14537445]\n",
            " [0.25110132]\n",
            " [0.1685022 ]\n",
            " [0.280837  ]\n",
            " [0.18722467]\n",
            " [0.20044053]\n",
            " [0.28964758]\n",
            " [0.1277533 ]\n",
            " [0.08039648]\n",
            " [0.18832599]\n",
            " [0.15638767]\n",
            " [0.21365639]\n",
            " [0.11123348]\n",
            " [0.15638767]\n",
            " [0.04295154]\n",
            " [0.14317181]\n",
            " [0.12444934]\n",
            " [0.1685022 ]\n",
            " [0.20154185]\n",
            " [0.13986784]\n",
            " [0.14647577]\n",
            " [0.17621145]\n",
            " [0.11123348]\n",
            " [0.28414097]\n",
            " [0.11674009]\n",
            " [0.24008811]\n",
            " [0.33590308]\n",
            " [0.16079295]\n",
            " [0.26321586]\n",
            " [0.15528634]\n",
            " [0.21035242]\n",
            " [0.27643172]\n",
            " [0.22136564]\n",
            " [0.23237885]\n",
            " [0.27422907]\n",
            " [0.18832599]\n",
            " [0.05396476]\n",
            " [0.11123348]\n",
            " [0.09471366]\n",
            " [0.02973568]\n",
            " [0.00660793]\n",
            " [0.00110132]\n",
            " [0.        ]\n",
            " [0.06718062]\n",
            " [0.06718062]\n",
            " [0.17511013]\n",
            " [0.08810573]\n",
            " [0.12885463]\n",
            " [0.1530837 ]\n",
            " [0.17070485]\n",
            " [0.06057269]\n",
            " [0.13546256]\n",
            " [0.04845815]\n",
            " [0.10022026]\n",
            " [0.18502203]\n",
            " [0.12004405]\n",
            " [0.1222467 ]\n",
            " [0.13876652]\n",
            " [0.12665198]\n",
            " [0.13325991]\n",
            " [0.08370044]\n",
            " [0.1376652 ]\n",
            " [0.15859031]\n",
            " [0.11453744]\n",
            " [0.05506608]\n",
            " [0.13656388]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD6CAYAAAC4RRw1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ntCn/P4095MpUuad7AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8o3bVkti34kj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#filling and eliminating missing data filling and eliminating missing data\n",
        "#I think this one is about dropping the time index column from the data\n",
        "from sklearn.impute import SimpleImputer\n",
        "from scipy.stats.mstats import winsorize\n",
        "imp = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
        "\n",
        "final_data = imp.fit_transform(datafive)\n",
        "#print(a)\n",
        "#datasix = datasix.drop(datasix.index[0])\n",
        "#Data_to_predict.values\n",
        "#datasix = datasix.dropna()\n",
        "#datafive = datafive.fillna(0)\n",
        "#unfortunately dropna() and fillna(0) are not very good solutions what if there were a better way?\n",
        "pca = PCA()# n_components=2 are the components in the dataset which we want to keep which have the greatest amount of correlation\n",
        "final_data = pca.fit_transform(final_data)\n",
        "\n",
        "#winsorization is for getting rid of outlier values in the data \n",
        "final_data = \twinsorize(final_data)\n",
        "\n",
        "\n",
        "final_data = pd.DataFrame(final_data)\n",
        "\n",
        "#this is basically selecting the first 224 values starting from the most initial value\n",
        "final_data_sub = final_data[224::]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q2mX2tCtO4gp",
        "colab_type": "code",
        "outputId": "db838b0c-806c-4a73-d7a9-8a3ac7e91a27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 400
        }
      },
      "source": [
        "#what if we want to use two or more datasets in order to predict values in the third dataset? or we want to a dataset which says what category that dataset is in? we can use pd.concat for that \n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "final_data= pd.concat([final_data,electricity],axis=1)# what this does is make sure that the datasets are connectet horizontally. so the other dataset basically becomes a second column in the new consolidated dataset. \n",
        "\n",
        "#if you want to do the splitting before putting it into a deep learning neural network then train_test_split is what is needed\n",
        "#whatever your y= will go here \n",
        "X_train, X_test, y_train, y_test = train_test_split(final_data, y, test_size=0.33, random_state=42)\n",
        "#.pca\n",
        "#you HAVE to convert your pandas dataframes into arrays before you can feed it into any ml or deep learning neural network or algorithm\n",
        "#higher order derivative(s) of values are easier for a machine learning algorithm to predict then the actual values\n",
        "electricity = np.array(electricity)\n",
        "#electricity = np.diff(electricity)\n",
        "final_data = np.array(final_data)\n",
        "final_data = np.diff(final_data)\n",
        "electricity = np.squeeze(electricity,axis=1)\n",
        "\n",
        "#generally you will only need to expand the number of dimensions of the dataset when you are working with RNNs or recurrent neural networks. \n",
        "electricity = np.expand_dims(electricity,axis=1)\n",
        "#this is dummy y data\n",
        "y = np.zeros(256)\n",
        "y = np.array(y)\n",
        "\n",
        "#again only something you are going to want to do if you are working with Recurrent neural networks\n",
        "X[:, None] \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-5e80a983f296>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfinal_data\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0melectricity\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m# what this does is make sure that the datasets are connectet horizontally. so the other dataset basically becomes a second column in the new consolidated dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#.pca\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, join_axes, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0mverify_integrity\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_integrity\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     )\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/pandas/core/reshape/concat.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, objs, axis, join, join_axes, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    330\u001b[0m                     \u001b[0;34m\" only Series and DataFrame objs are valid\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m                 )\n\u001b[0;32m--> 332\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;31m# consolidate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: cannot concatenate object of type '<class 'numpy.ndarray'>'; only Series and DataFrame objs are valid"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BRBu-Ha54oNW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        },
        "outputId": "c3e1b4bc-216c-4d5a-aa39-96f14147e8e1"
      },
      "source": [
        "#the above is generally not needed however because tensorflow allows you to split it during the fitting phase and once can simply manually slice\n",
        "#the arrays \n",
        "#model.fit(X,Y,epochs=700, verbose=1,validation_split=0.2)\n",
        "\n",
        "final_data = np.squeeze(final_data)\n",
        "plt.plot(final_data)\n",
        "#if enough time reamains go into various data visualization such as data decomposition, graphing and confidence intervals sns library. "
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-49e4ad4f59b0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mfinal_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#if enough time reamains go into various data visualization such as data decomposition, graphing and confidence intervals sns library.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T2uGhlfEByPq",
        "colab_type": "text"
      },
      "source": [
        "Part two starts from here "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYRgvzpgB0xl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 817
        },
        "outputId": "5a04ba43-1233-442a-f99f-360c466e9134"
      },
      "source": [
        "!pip install tensorflow==2.0.0-beta1\n",
        "#!pip install tensorflow==2.0.0rc0\n",
        "#!pip install tensorflow-probability\n",
        "#!pip install gaussian_processes\n",
        "#!pip install quandl\n",
        "#!pip install rpy2\n",
        "#!pip install bootstrapped\n",
        "!pip install hurst"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/29/6c/2c9a5c4d095c63c2fb37d20def0e4f92685f7aee9243d6aae25862694fd1/tensorflow-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (87.9MB)\n",
            "\u001b[K     |████████████████████████████████| 87.9MB 105kB/s \n",
            "\u001b[?25hRequirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.2.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.17.5)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.1.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.8.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.27.1)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 35.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.34.2)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 39.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (3.10.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (0.9.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==2.0.0-beta1) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow==2.0.0-beta1) (45.1.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow\n",
            "  Found existing installation: tensorflow 1.15.0\n",
            "    Uninstalling tensorflow-1.15.0:\n",
            "      Successfully uninstalled tensorflow-1.15.0\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorboard",
                  "tensorflow",
                  "tensorflow_estimator"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Collecting hurst\n",
            "  Downloading https://files.pythonhosted.org/packages/02/4f/d3471ce0dca03a21d4c6640da07a6040c9cc800a937233086b6cea6a7dc2/hurst-0.0.5-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.10 in /usr/local/lib/python3.6/dist-packages (from hurst) (1.17.5)\n",
            "Requirement already satisfied: pandas>=0.18 in /usr/local/lib/python3.6/dist-packages (from hurst) (0.25.3)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->hurst) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.18->hurst) (2.6.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.6.1->pandas>=0.18->hurst) (1.12.0)\n",
            "Installing collected packages: hurst\n",
            "Successfully installed hurst-0.0.5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6vP7mnY6B4rr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "#import quandl\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.decomposition import PCA, KernelPCA\n",
        "#import sherpa"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92kvs2q9VxjV",
        "colab_type": "text"
      },
      "source": [
        "Now we will do classifcation in the functional instead of sequential form. The functional form is considered more powerful because it among other things allows for more flexibility such as connecting layers at different levels to on another. \n",
        "\n",
        "Note: I use MSLE and ADagrad because those are the optimizers and losses which work best for me. However the standard \"default optimizer and loss to use is MSE and ADAM respectively for regression and binary crossentropy for 2 category classifcation algorithms and categorical crossentropy as the default loss function for classification algorithms with more then two buckets\n",
        "\n",
        "Take a look at the last layer of the neural network. One of the things which must be kept in mind is that the number of nodes in a neural netowork of the last layer MUST equal the number of buckets for our data. Also a sigmoid activation function is generally the standard activation function to use in the final layer of classifcation neural network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUDKBHkasbSd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "c474fde5-a491-4bba-bbd1-c780b0d23068"
      },
      "source": [
        "from sklearn.datasets import load_iris, fetch_california_housing\n",
        "\n",
        "data = load_iris()\n",
        "print(data)\n",
        "print(data['target'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
            "       [4.9, 3. , 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.3, 0.2],\n",
            "       [4.6, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.6, 1.4, 0.2],\n",
            "       [5.4, 3.9, 1.7, 0.4],\n",
            "       [4.6, 3.4, 1.4, 0.3],\n",
            "       [5. , 3.4, 1.5, 0.2],\n",
            "       [4.4, 2.9, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.1],\n",
            "       [5.4, 3.7, 1.5, 0.2],\n",
            "       [4.8, 3.4, 1.6, 0.2],\n",
            "       [4.8, 3. , 1.4, 0.1],\n",
            "       [4.3, 3. , 1.1, 0.1],\n",
            "       [5.8, 4. , 1.2, 0.2],\n",
            "       [5.7, 4.4, 1.5, 0.4],\n",
            "       [5.4, 3.9, 1.3, 0.4],\n",
            "       [5.1, 3.5, 1.4, 0.3],\n",
            "       [5.7, 3.8, 1.7, 0.3],\n",
            "       [5.1, 3.8, 1.5, 0.3],\n",
            "       [5.4, 3.4, 1.7, 0.2],\n",
            "       [5.1, 3.7, 1.5, 0.4],\n",
            "       [4.6, 3.6, 1. , 0.2],\n",
            "       [5.1, 3.3, 1.7, 0.5],\n",
            "       [4.8, 3.4, 1.9, 0.2],\n",
            "       [5. , 3. , 1.6, 0.2],\n",
            "       [5. , 3.4, 1.6, 0.4],\n",
            "       [5.2, 3.5, 1.5, 0.2],\n",
            "       [5.2, 3.4, 1.4, 0.2],\n",
            "       [4.7, 3.2, 1.6, 0.2],\n",
            "       [4.8, 3.1, 1.6, 0.2],\n",
            "       [5.4, 3.4, 1.5, 0.4],\n",
            "       [5.2, 4.1, 1.5, 0.1],\n",
            "       [5.5, 4.2, 1.4, 0.2],\n",
            "       [4.9, 3.1, 1.5, 0.2],\n",
            "       [5. , 3.2, 1.2, 0.2],\n",
            "       [5.5, 3.5, 1.3, 0.2],\n",
            "       [4.9, 3.6, 1.4, 0.1],\n",
            "       [4.4, 3. , 1.3, 0.2],\n",
            "       [5.1, 3.4, 1.5, 0.2],\n",
            "       [5. , 3.5, 1.3, 0.3],\n",
            "       [4.5, 2.3, 1.3, 0.3],\n",
            "       [4.4, 3.2, 1.3, 0.2],\n",
            "       [5. , 3.5, 1.6, 0.6],\n",
            "       [5.1, 3.8, 1.9, 0.4],\n",
            "       [4.8, 3. , 1.4, 0.3],\n",
            "       [5.1, 3.8, 1.6, 0.2],\n",
            "       [4.6, 3.2, 1.4, 0.2],\n",
            "       [5.3, 3.7, 1.5, 0.2],\n",
            "       [5. , 3.3, 1.4, 0.2],\n",
            "       [7. , 3.2, 4.7, 1.4],\n",
            "       [6.4, 3.2, 4.5, 1.5],\n",
            "       [6.9, 3.1, 4.9, 1.5],\n",
            "       [5.5, 2.3, 4. , 1.3],\n",
            "       [6.5, 2.8, 4.6, 1.5],\n",
            "       [5.7, 2.8, 4.5, 1.3],\n",
            "       [6.3, 3.3, 4.7, 1.6],\n",
            "       [4.9, 2.4, 3.3, 1. ],\n",
            "       [6.6, 2.9, 4.6, 1.3],\n",
            "       [5.2, 2.7, 3.9, 1.4],\n",
            "       [5. , 2. , 3.5, 1. ],\n",
            "       [5.9, 3. , 4.2, 1.5],\n",
            "       [6. , 2.2, 4. , 1. ],\n",
            "       [6.1, 2.9, 4.7, 1.4],\n",
            "       [5.6, 2.9, 3.6, 1.3],\n",
            "       [6.7, 3.1, 4.4, 1.4],\n",
            "       [5.6, 3. , 4.5, 1.5],\n",
            "       [5.8, 2.7, 4.1, 1. ],\n",
            "       [6.2, 2.2, 4.5, 1.5],\n",
            "       [5.6, 2.5, 3.9, 1.1],\n",
            "       [5.9, 3.2, 4.8, 1.8],\n",
            "       [6.1, 2.8, 4. , 1.3],\n",
            "       [6.3, 2.5, 4.9, 1.5],\n",
            "       [6.1, 2.8, 4.7, 1.2],\n",
            "       [6.4, 2.9, 4.3, 1.3],\n",
            "       [6.6, 3. , 4.4, 1.4],\n",
            "       [6.8, 2.8, 4.8, 1.4],\n",
            "       [6.7, 3. , 5. , 1.7],\n",
            "       [6. , 2.9, 4.5, 1.5],\n",
            "       [5.7, 2.6, 3.5, 1. ],\n",
            "       [5.5, 2.4, 3.8, 1.1],\n",
            "       [5.5, 2.4, 3.7, 1. ],\n",
            "       [5.8, 2.7, 3.9, 1.2],\n",
            "       [6. , 2.7, 5.1, 1.6],\n",
            "       [5.4, 3. , 4.5, 1.5],\n",
            "       [6. , 3.4, 4.5, 1.6],\n",
            "       [6.7, 3.1, 4.7, 1.5],\n",
            "       [6.3, 2.3, 4.4, 1.3],\n",
            "       [5.6, 3. , 4.1, 1.3],\n",
            "       [5.5, 2.5, 4. , 1.3],\n",
            "       [5.5, 2.6, 4.4, 1.2],\n",
            "       [6.1, 3. , 4.6, 1.4],\n",
            "       [5.8, 2.6, 4. , 1.2],\n",
            "       [5. , 2.3, 3.3, 1. ],\n",
            "       [5.6, 2.7, 4.2, 1.3],\n",
            "       [5.7, 3. , 4.2, 1.2],\n",
            "       [5.7, 2.9, 4.2, 1.3],\n",
            "       [6.2, 2.9, 4.3, 1.3],\n",
            "       [5.1, 2.5, 3. , 1.1],\n",
            "       [5.7, 2.8, 4.1, 1.3],\n",
            "       [6.3, 3.3, 6. , 2.5],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [7.1, 3. , 5.9, 2.1],\n",
            "       [6.3, 2.9, 5.6, 1.8],\n",
            "       [6.5, 3. , 5.8, 2.2],\n",
            "       [7.6, 3. , 6.6, 2.1],\n",
            "       [4.9, 2.5, 4.5, 1.7],\n",
            "       [7.3, 2.9, 6.3, 1.8],\n",
            "       [6.7, 2.5, 5.8, 1.8],\n",
            "       [7.2, 3.6, 6.1, 2.5],\n",
            "       [6.5, 3.2, 5.1, 2. ],\n",
            "       [6.4, 2.7, 5.3, 1.9],\n",
            "       [6.8, 3. , 5.5, 2.1],\n",
            "       [5.7, 2.5, 5. , 2. ],\n",
            "       [5.8, 2.8, 5.1, 2.4],\n",
            "       [6.4, 3.2, 5.3, 2.3],\n",
            "       [6.5, 3. , 5.5, 1.8],\n",
            "       [7.7, 3.8, 6.7, 2.2],\n",
            "       [7.7, 2.6, 6.9, 2.3],\n",
            "       [6. , 2.2, 5. , 1.5],\n",
            "       [6.9, 3.2, 5.7, 2.3],\n",
            "       [5.6, 2.8, 4.9, 2. ],\n",
            "       [7.7, 2.8, 6.7, 2. ],\n",
            "       [6.3, 2.7, 4.9, 1.8],\n",
            "       [6.7, 3.3, 5.7, 2.1],\n",
            "       [7.2, 3.2, 6. , 1.8],\n",
            "       [6.2, 2.8, 4.8, 1.8],\n",
            "       [6.1, 3. , 4.9, 1.8],\n",
            "       [6.4, 2.8, 5.6, 2.1],\n",
            "       [7.2, 3. , 5.8, 1.6],\n",
            "       [7.4, 2.8, 6.1, 1.9],\n",
            "       [7.9, 3.8, 6.4, 2. ],\n",
            "       [6.4, 2.8, 5.6, 2.2],\n",
            "       [6.3, 2.8, 5.1, 1.5],\n",
            "       [6.1, 2.6, 5.6, 1.4],\n",
            "       [7.7, 3. , 6.1, 2.3],\n",
            "       [6.3, 3.4, 5.6, 2.4],\n",
            "       [6.4, 3.1, 5.5, 1.8],\n",
            "       [6. , 3. , 4.8, 1.8],\n",
            "       [6.9, 3.1, 5.4, 2.1],\n",
            "       [6.7, 3.1, 5.6, 2.4],\n",
            "       [6.9, 3.1, 5.1, 2.3],\n",
            "       [5.8, 2.7, 5.1, 1.9],\n",
            "       [6.8, 3.2, 5.9, 2.3],\n",
            "       [6.7, 3.3, 5.7, 2.5],\n",
            "       [6.7, 3. , 5.2, 2.3],\n",
            "       [6.3, 2.5, 5. , 1.9],\n",
            "       [6.5, 3. , 5.2, 2. ],\n",
            "       [6.2, 3.4, 5.4, 2.3],\n",
            "       [5.9, 3. , 5.1, 1.8]]), 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
            "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]), 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'), 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...', 'feature_names': ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)'], 'filename': '/usr/local/lib/python3.6/dist-packages/sklearn/datasets/data/iris.csv'}\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7sHudVXFP7p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "75623f3d-8da5-46c8-9bee-2bc3f90c7341"
      },
      "source": [
        "#DNN for regression\n",
        "\n",
        "X_full, y_full = fetch_california_housing(return_X_y=True)\n",
        "print(X_full.shape)\n",
        "print(y_full.shape)\n",
        "#Scalar = MinMaxScaler(feature_range=(0,1))\n",
        "#X_full = Scalar.fit_transform(X_full)\n",
        "#y_full = Scalar.fit_transform(y_full)\n",
        "\n",
        "pca = PCA()# n_components=2 are the components in the dataset which we want to keep which have the greatest amount of correlation\n",
        "#X_full = Scalar.fit_transform(X_full)\n",
        "#y_full = Scalar.fit_transform(y_full)\n",
        "X_full = pca.fit_transform(X_full)\n",
        "#\n",
        "print(X_full)\n",
        "print(X_full.shape)\n",
        "model = tf.keras.Sequential([\n",
        "\ttf.keras.Input(shape=(8,)),\n",
        "  tf.keras.layers.Dense(25,kernel_initializer='ones', use_bias=False),\n",
        "  #tf.keras.layers.InputLayer(input_shape=(10),dtype=x.dtype),#put a 1 before the 9 later\n",
        "  tf.keras.layers.Dense(50,kernel_initializer='ones', use_bias=False),\n",
        "  tf.keras.layers.BatchNormalization(),\n",
        "  tf.keras.layers.Dense(25,kernel_initializer='ones', use_bias=False)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='Adam',\n",
        "        loss='MSLE')\n",
        "\t\n",
        "model.fit(X_full,y_full,epochs=100, verbose=1,validation_split=0.2)#, callbacks=[early_stop])\n",
        "\n",
        "Prediction = model.predict(X_full[120::])\n",
        "print(Prediction)\n",
        "print(Prediction.shape)\n",
        "u = 219\n",
        "model.summary()\n",
        "\n",
        "plt.plot(Prediction)\n",
        "plt.plot(y_full[120::])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n",
            "[[-1.10351265e+03  8.56663624e+00 -7.74104267e-01 ... -3.74666242e+00\n",
            "  -1.33840490e-01  8.91889722e-02]\n",
            " [ 9.75541244e+02 -4.67041774e+00 -1.02568060e+00 ... -3.91094740e+00\n",
            "  -5.61873090e-03  1.79110798e-01]\n",
            " [-9.29549908e+02  2.00346509e+01 -1.97273701e+00 ... -2.26989335e+00\n",
            "  -6.97511796e-02 -2.24880496e-01]\n",
            " ...\n",
            " [-4.18437575e+02 -1.29082558e+01  1.01901295e+00 ...  1.75428879e+00\n",
            "  -9.97278741e-01 -2.89576910e-03]\n",
            " [-6.84439525e+02 -1.27945805e+01  9.74409305e-01 ...  1.63168668e+00\n",
            "  -9.33492785e-01  3.57550397e-02]\n",
            " [-3.84362474e+01 -1.26703781e+01  1.04286818e+00 ...  1.17996360e+00\n",
            "  -1.01309763e+00  9.46645875e-02]]\n",
            "(20640, 8)\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Train on 16512 samples, validate on 4128 samples\n",
            "Epoch 1/100\n",
            "16512/16512 [==============================] - 1s 83us/sample - loss: 2.2686 - val_loss: 1.5135\n",
            "Epoch 2/100\n",
            "16512/16512 [==============================] - 1s 61us/sample - loss: 1.2884 - val_loss: 1.3430\n",
            "Epoch 3/100\n",
            "16512/16512 [==============================] - 1s 62us/sample - loss: 1.1463 - val_loss: 1.2904\n",
            "Epoch 4/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 1.0115 - val_loss: 0.8650\n",
            "Epoch 5/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.2146 - val_loss: 0.1429\n",
            "Epoch 6/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.1232 - val_loss: 0.1431\n",
            "Epoch 7/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.1232 - val_loss: 0.1430\n",
            "Epoch 8/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.1231 - val_loss: 0.1434\n",
            "Epoch 9/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.1231 - val_loss: 0.1451\n",
            "Epoch 10/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.1231 - val_loss: 0.1459\n",
            "Epoch 11/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.1232 - val_loss: 0.1464\n",
            "Epoch 12/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.1232 - val_loss: 0.1454\n",
            "Epoch 13/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.1233 - val_loss: 0.1462\n",
            "Epoch 14/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.1232 - val_loss: 0.1404\n",
            "Epoch 15/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.1233 - val_loss: 0.1455\n",
            "Epoch 16/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.1231 - val_loss: 0.1437\n",
            "Epoch 17/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.1232 - val_loss: 0.1420\n",
            "Epoch 18/100\n",
            "16512/16512 [==============================] - 1s 62us/sample - loss: 0.1232 - val_loss: 0.1462\n",
            "Epoch 19/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.1232 - val_loss: 0.1435\n",
            "Epoch 20/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.1233 - val_loss: 0.1427\n",
            "Epoch 21/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.1232 - val_loss: 0.1436\n",
            "Epoch 22/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.1232 - val_loss: 0.1451\n",
            "Epoch 23/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.1231 - val_loss: 0.1456\n",
            "Epoch 24/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.1231 - val_loss: 0.1404\n",
            "Epoch 25/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.1232 - val_loss: 0.1474\n",
            "Epoch 26/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.1233 - val_loss: 0.1471\n",
            "Epoch 27/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.1232 - val_loss: 0.1463\n",
            "Epoch 28/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.1232 - val_loss: 0.1438\n",
            "Epoch 29/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.1232 - val_loss: 0.1462\n",
            "Epoch 30/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.1232 - val_loss: 0.1466\n",
            "Epoch 31/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.1232 - val_loss: 0.1467\n",
            "Epoch 32/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.1232 - val_loss: 0.1441\n",
            "Epoch 33/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.1233 - val_loss: 0.1447\n",
            "Epoch 34/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0937 - val_loss: 0.1313\n",
            "Epoch 35/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0596 - val_loss: 0.0681\n",
            "Epoch 36/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0581 - val_loss: 0.0750\n",
            "Epoch 37/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0557 - val_loss: 0.0546\n",
            "Epoch 38/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0546 - val_loss: 0.0544\n",
            "Epoch 39/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0548 - val_loss: 0.0650\n",
            "Epoch 40/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0545 - val_loss: 0.0479\n",
            "Epoch 41/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0545 - val_loss: 0.0822\n",
            "Epoch 42/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0530 - val_loss: 0.0494\n",
            "Epoch 43/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0529 - val_loss: 0.0477\n",
            "Epoch 44/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0536 - val_loss: 0.0536\n",
            "Epoch 45/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0518 - val_loss: 0.0494\n",
            "Epoch 46/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0528 - val_loss: 0.0452\n",
            "Epoch 47/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0523 - val_loss: 0.0518\n",
            "Epoch 48/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0520 - val_loss: 0.0466\n",
            "Epoch 49/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0526 - val_loss: 0.0548\n",
            "Epoch 50/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0517 - val_loss: 0.0634\n",
            "Epoch 51/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0534 - val_loss: 0.0475\n",
            "Epoch 52/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0519 - val_loss: 0.0691\n",
            "Epoch 53/100\n",
            "16512/16512 [==============================] - 1s 62us/sample - loss: 0.0540 - val_loss: 0.0584\n",
            "Epoch 54/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0520 - val_loss: 0.0508\n",
            "Epoch 55/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0530 - val_loss: 0.0838\n",
            "Epoch 56/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0517 - val_loss: 0.0751\n",
            "Epoch 57/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0523 - val_loss: 0.0447\n",
            "Epoch 58/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0522 - val_loss: 0.0450\n",
            "Epoch 59/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0517 - val_loss: 0.0539\n",
            "Epoch 60/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0514 - val_loss: 0.0782\n",
            "Epoch 61/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0514 - val_loss: 0.0669\n",
            "Epoch 62/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0517 - val_loss: 0.0775\n",
            "Epoch 63/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0536 - val_loss: 0.0481\n",
            "Epoch 64/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0525 - val_loss: 0.0449\n",
            "Epoch 65/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0524 - val_loss: 0.0447\n",
            "Epoch 66/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0514 - val_loss: 0.0574\n",
            "Epoch 67/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0519 - val_loss: 0.0719\n",
            "Epoch 68/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0517 - val_loss: 0.0537\n",
            "Epoch 69/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0515 - val_loss: 0.0544\n",
            "Epoch 70/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0528 - val_loss: 0.0526\n",
            "Epoch 71/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0517 - val_loss: 0.0715\n",
            "Epoch 72/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0518 - val_loss: 0.0526\n",
            "Epoch 73/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0520 - val_loss: 0.0438\n",
            "Epoch 74/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0521 - val_loss: 0.0447\n",
            "Epoch 75/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0510 - val_loss: 0.0450\n",
            "Epoch 76/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0520 - val_loss: 0.0570\n",
            "Epoch 77/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0515 - val_loss: 0.0458\n",
            "Epoch 78/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0520 - val_loss: 0.0763\n",
            "Epoch 79/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0514 - val_loss: 0.0644\n",
            "Epoch 80/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0522 - val_loss: 0.0725\n",
            "Epoch 81/100\n",
            "16512/16512 [==============================] - 1s 61us/sample - loss: 0.0510 - val_loss: 0.0828\n",
            "Epoch 82/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0514 - val_loss: 0.0496\n",
            "Epoch 83/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0525 - val_loss: 0.0456\n",
            "Epoch 84/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0520 - val_loss: 0.0459\n",
            "Epoch 85/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0521 - val_loss: 0.0477\n",
            "Epoch 86/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0518 - val_loss: 0.0782\n",
            "Epoch 87/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0524 - val_loss: 0.0450\n",
            "Epoch 88/100\n",
            "16512/16512 [==============================] - 1s 67us/sample - loss: 0.0515 - val_loss: 0.0836\n",
            "Epoch 89/100\n",
            "16512/16512 [==============================] - 1s 68us/sample - loss: 0.0526 - val_loss: 0.0732\n",
            "Epoch 90/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0521 - val_loss: 0.0826\n",
            "Epoch 91/100\n",
            "16512/16512 [==============================] - 1s 66us/sample - loss: 0.0526 - val_loss: 0.0466\n",
            "Epoch 92/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0514 - val_loss: 0.0709\n",
            "Epoch 93/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0515 - val_loss: 0.0600\n",
            "Epoch 94/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0522 - val_loss: 0.0820\n",
            "Epoch 95/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0514 - val_loss: 0.0542\n",
            "Epoch 96/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0523 - val_loss: 0.0441\n",
            "Epoch 97/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0511 - val_loss: 0.0534\n",
            "Epoch 98/100\n",
            "16512/16512 [==============================] - 1s 63us/sample - loss: 0.0514 - val_loss: 0.0526\n",
            "Epoch 99/100\n",
            "16512/16512 [==============================] - 1s 64us/sample - loss: 0.0513 - val_loss: 0.0508\n",
            "Epoch 100/100\n",
            "16512/16512 [==============================] - 1s 65us/sample - loss: 0.0518 - val_loss: 0.0786\n",
            "[[2.3987584 2.3987584 2.3987575 ... 2.3987584 2.3987584 2.3987582]\n",
            " [2.207266  2.207266  2.2072666 ... 2.207266  2.207266  2.207266 ]\n",
            " [2.439226  2.439226  2.4392245 ... 2.439226  2.4392257 2.4392262]\n",
            " ...\n",
            " [1.3627403 1.3627403 1.3627406 ... 1.3627404 1.3627404 1.3627403]\n",
            " [1.4199306 1.4199306 1.4199312 ... 1.4199307 1.4199307 1.4199307]\n",
            " [1.4801012 1.4801012 1.4801013 ... 1.4801013 1.4801012 1.4801013]]\n",
            "(20520, 25)\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 25)                200       \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 50)                1250      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 50)                200       \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 25)                1250      \n",
            "=================================================================\n",
            "Total params: 2,900\n",
            "Trainable params: 2,800\n",
            "Non-trainable params: 100\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7f381a0434e0>]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX8AAAD4CAYAAAAEhuazAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0\ndHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAfrklEQVR4nO3deXwU9f3H8dcnN0e4EQIBwiUVqShE\nxf60nnjVVi1V8UBEEQ+goFLr2aqttdbWEwE5vNAKWg/AGyxVa0UJyo0gIPd9Jpg7+f7+2EnYHJuE\nhGWTzPv5eOTBzHeu73yZfe/sd2d2zDmHiIj4S1SkKyAiIkeewl9ExIcU/iIiPqTwFxHxIYW/iIgP\nxUS6AlXRqlUrl5KSEulqiIjUKQsWLNjlnGtd3rQ6Ef4pKSmkpaVFuhoiInWKma0PNU3dPiIiPqTw\nFxHxIYW/iIgPKfxFRHxI4S8i4kMKfxERH1L4i4j4UJ24zr8mCgsLMAyLOvg+t/iTD5kzeRyjpr5J\ndExsifkXvPcOycf0ok2XbiHXmbFnFxNvuY62Xbtz9V+eACD7wAHGP3AHlhPL6GfGHvb9WDp3Nh9N\neIohT0ygRbtksg5ksCbtK1p16MTebVv47NUXOLB7F5fd/zBfTH+FNl27cWD3bi4a/XssKooln3zE\nl29N48DuXRx9ymm06dyVrn1PpmVyhxLbWf75XD4Y+w8AWiZ3pGmbtiQf04tuqSez4r+f0qpjJzoe\n25t927fStmt3dm1YR1RMDAmNGhMVE0NhQQGvP3g3J/5qAG27duftRx8krmEjdq5byzlDb+XY08/h\n3y9MoP+NI0r8nxyqfdu30ahpMzauWMKij9/n6H6ncuzpZwOwZ8sm3vrrA+zfvo0zrxvG3Bcnllh2\n8GNjWb9kER17Hce2td/z8YSnARj54uvENWhYPN/uTRt49Z7bufqRJ2jZ/mA7bVi6mNYpnSnMz2fT\niqW8++SjAPzfFYNY8N47ZB/IAODs62/h+PN+webvljN/1ltcOPIO4hIalKhLblYm/7xvDJfd/zCN\nmjUvd1+3/7CG9F072LNpI845uvY9iU3fLePY088us77y5GVns/m7ZaQc35d927aSm53FUSldyrRn\nTuaPtOncFYD1SxbSpks3Eho1LtEen/3zRdYu+Jr+w0ZwzP+dQWxCAs451iz4ms7H9yU65mCkZB84\nwP/eeJXVafM449qhHH3y/7H5u+UktmrNpOFDiuc74fxfctaQm1j55ee8++Sj9P3FxSx4bwbDxr9I\nYotWZfZn47LFtOqYQoPEJmQdyGDcDVfSuGUrstPTadSiBfu3byue99rHxtK8bTs2LFtElxNODOzb\n4oUktmrN6vlf0uuMc0honMjE4UNo3akzKcf1YduaVXz3xac0aNKUs6+/BecKadule7ntdjj88G0a\nW1atIComhgXvvkNO5o/Fx044WV34Pf/U1FRX3Zu8nho0gMSWLbn+yUAAHNi7h+duvhaA5kntGfLE\nBFxhIVHR0Sz79BM+HBcI82HjXiSxZdkDzznH4wN/WTw+fMo0Eho35rX7f8eWVSuIbzaSEc+dV626\nhrJo9vvMmTyuePym8S/x3C2Dq7TsRaPvIi8nm4/GP1nu9Dumv1s8XFhQwBNXXVzlel33+HhevP2W\n4vGY2Djy83KrtGyPn/2ci0bdWel8G5YuJjYhnqRuPcjLziYvN4fxN14NQMOmzcjcv6943qJ9mTTi\nBtJ3bq/yfgQLbo9/XHFRmfL0XTuYNPx6GjdvwYG9eypd3+3TZhUfLym9+zDgnodKTA/exogXXufT\nqZM549qhJd6EgucJFtyGrrCQ77/+H91P+lmZN9VZT/yVVfP+yw1PTWLKqBvL7GfwNu6Y/m5xoHbs\n1ZvL7n8YgLzcHJ4eNKDEMmZROFdISu8+rFv0DSdfegWnDhxUPH3c0KvIykgvHr9j+rsh96XTcSew\nfvG3ZcqD67lj3Vqm/v63ALTu1Jlr//ZMyPUF63XmuSyd+zHXPPIkjZq3KH79AzRrk8TPLruK970T\nnsqUbrdQsn88QGx8PBuWLKJ1ShcaN28Rct5Q+1DVbVXEzBY451LLm1bvz/zzc3PYu3VL8Xjwf/ze\nrZuZ++JEvv1wFqNeebs4+AEm3npduY3vCgtLjO/auI783Fy2rFpRNAc7tm7lqKSkw7YP/3lpconx\nz197qcrLLv/sE5K69ajSvAUF+YdUr+DgB6oc/AC7Nqyr0nxv/OkeIPBCmHrXKPZu3Vw8LTj4g1U3\n+Cuy/PO5dEs9mW8/DBwTVQl+gG/en1k8vK5UuOVk/lhifOyQywHI2LObAXc/WOm6t61ZxaqvviC+\nYSO2fr+SL6ZP5czBN3LsGecQ37AR+7ZvIyY2llXz/gtQHPyVyc/NAWD35o3FZRuWLCozn3OB18K6\nRd8AsH9H4Iy7ID+PrPT0EsEPMP3Bu0Jus7zgL23pf2YXD+9c/0OVgh9g6dyPAXjl7tFlpu3bvrXK\nwQ/w+kP3cPkf/gLAllXf0aJ9cvGnoz1bNpPYsiWx8Qk8e/1Aknv2YtPypTRPald88nkosjLSaZDY\n5JCXq6p6Hf45mZnFw28/+iAbly8tM8+3H84C4JW7RoVcz/a1q2ncoiWNmjXntT/8rsS0Lau+4/N/\nvliibNeObRyVlMT3X/2P9j/pScOmzaq9D8s+/aRMqC7/7N9VXn7tN/MrDf83H/kjPfqdytcL51Wr\njtWxe9MG9m3bSrO2STjn+Ob9GfQ6sz/xDRsVz1OQf/DNKGPPrhLBH26bV64oMV7UFVZRd2B5/vPy\npOLh2Lh43vjzfeTn5NDrrP7F3U2lrVu4AIDPZr/H/MnjQ657//ZtzHr8kRJlc1+axNyXJnHxmPuY\n8fc/h1zWFRYWf0IoyM8rNTHwT1b6fvLz8oiJjSW31BtVqHUCPHn1peVO31TO66+u2bhsMRDoTn7t\n/jEkde/B6YOGsmXlcj579QUATrvqOuDg/u7dtjXk+tYs+DrktHFDrzosZ/+h1OvwLww6k137zfwK\n5929aUPIaa/cPZrYhAacfMllbFu9qsS0L958rcS4K0wnIz2O3OwsZj7+F5omtWNoBe/6z950O1mJ\n2YGR2Chuf/gpCvLyiI1PACjxaaS6vnh/VshpzjnWLVwQCJzOHWu8rUMxZ8o41i/+lt7n/oJFH7/H\n1tWrSnQFBYfX1N+HfnMu8o8rLuL0QTfUqE6FBQVERUczrdSbfJHta1dXe915OdlsWLIQIOiTYvmq\nelYbSkXBD/Dj/n3FXRGFQZ9m5740iePPvTBQXlDAuKFX0bv/BaTNeqvSbe7dvo0XSn0aPJz2bN4U\ntnUfqqI3uq3fryxzrJQ+GcS54jfR0pZ/PrfC7axfvJBOxx1fo7qGUq/7/F9+9m/s/Oyzam+3SYO+\n5PXIJ2th2Y+8IVkjXLum9D71TBZPfxWAC+75E+9PncSYv48rM/szQ1/BoppQmL8di2pAbsEsyPyR\nUVPf4qk/3AY/hPxdpiqLjvspBblLyp1222szeOJKr5+/U0dYH/pN8EgY9OQktm5aQ+8TT61xAFZH\nrzPPpdd5v2TaXSOP+LaPpKjoaDqecipRrpANi9eQn7Gl8oUiICruJ7i8H3AuJ9JVKXbH9HcZO2wE\nOfvXHdJyFtWCmMTGxGQmkdl+G2MefZa/D/o1lltxd2lNzv4r6vOv1+H/zJ/vJHfJ8mpvN7bRRVj0\nUeSmP3/Iy0bH96YgJ/CmEddkEBTmEBWbXGIe53LI2fdsuctbVAtcYdX6lWsipuHZ5Gd+AkB802Hk\n7D/0vsnDLTq+D9FxPcjNeK3ymUWOsITGXcg+sLZ6C1siuAyiYo8mpsFp5KZPqXQR16FjuSeOVdqc\nX8Mfav7xubqiYntQmLcyItuurqjYrhTmrYl0NQ6L+CZDISoRV7CL3IypRMefQEFO5V8qSt0U32wU\nOfueKh4PPqmpiuj4vkTH9SQ3YyoAcYkDyc2YdtjrWR3Rcb0YPfWv1Vq2ovCP2E1eZna+ma00s9Vm\nFvoygDrqSAW/xQa+tmlxwgkkxNSsb7Ao+KOiDl5maK0PXrV00jXXQUKj0otVmSU2w6LbVHv5qko8\nqg0jJl3CiOfOZuTkK2iYeAyxDc8kvtltYd+21EDrtri21Tg+zBjxXP+Do9HRdO/ftMqLFyYnc8I1\nJzJy8hXFZSMnX1P8vVukFeSG54vyiHzha2bRwLNAf2ATMN/MZjrnqt9HE8LQZ6YweWTJLwEvHDmG\n+bPeIr5BQ37ct4cOPY+j/7ARxdMr+7Rw7s2/DXmlRrj89qV/EZtw8GD8cd9eZk8aywXDby9xhQzA\nxxOfYcknH3Hjs8/z8uj7yck7tKtkTrlsAF9Mn4pZFLePnUROZiabViyla9+TOO2XvynTPv0GDOSU\n31yJYTx+5a8AGDX1LWLi4sjLzubdpx5l7TfzuXzM3ST/5Fi2rFpBi3YdSGjcmHFXjyEr/7sK6zP6\n1Xf4YvpUUnr3JfvHDLqd2I+s9HQWvPcO82e+CUC3E/vRf9hIGjYJ/aKPy/2U8nqOk7r3YOv3od+s\nU3r3IWP3rjIXBVw85j66ndiP/Nxcnhr06zLLFV1X36XPiVxy5x/I2L2LF267mfzcHM6/9Ta+eP0V\nhvxjPE8P/g1Q8XXwtUXrlC7k5+ayd0voL19jExqQl50FBG4UbJ7UntXzv6xwvUX92kU3jM147E9V\nqs/Prx7Cib8K3H/ws8uvZsvKFQy45yEO7N3Dihklv6S+Y/q7zJkynkUfv8eoV97mqWsCVyXd/ugz\nZW72BLh1ymvkZmUy4a7b6dCtKxu++l+l9WmZ3LHCi0eqJgoorHSumopIt4+ZnQI84Jw7zxu/G8A5\n90h589ek2wcClwkmNGpM+s6dtGjXvtI7S/Oys/n31CksnfMBAI1btuKSMffRqmMnwIiOiSE3K5Po\n2DjWLPiq+HK7gQ89FvIqkZqIaXgOo14oe41yKAX5eaTv3EHzpPZs+GEV7057iSuuvRkzo1nbJKKi\nolnx+VzWL1nI+beWfza8ZsFXtEzuRLM2bctMe+PP97FhyUISmt+OsxmMGH/w43ZWRjqFBQUl7lbN\nzc5i5Zef0+uM/pjZIez54TFh6J0UxJxPbM5chr0QCJXHr7waV7gfgNGvvs3KL//LjnVr+ea9GcXX\nr9804WUaJDYpcdcqBC7vNTOSupe8hHbxnA9p06UbjZo1p2HTZkRFR5epy+5NG1n22SecduXg4rZY\n9uknNGrWnJTefYDAHcov3HYzl//xEY5K6crLNz9OToNTKMh9hfzMnSXWZ1FRxVeedE3tx5q0efQf\nNpKfnnUuz978ETn7AnebD3r0aY5K6cKzNwcuEx70558y6/G/sG3N96T07kNK774lLksFGP78tOJr\n2F9/6B42LlvM0Gcm0/SowDGRsXtX4G7qaVNolJlLpx49OePaG4lv2JCKpO/awbJPP6HDMT/ly7em\nERufwCW/u6/EPMFvgo3ijiXLZRPboIArH/g9r9zzMvk5i7l18mQaNE4MuZ2iO6vXpM3jrCE3ccL5\nvywxff3ihWxcvphTBx6892f+rLcozM/n5EsvL1vvnTuYNOL64vFL7/ojHzyXgXOOQX/qRWxCQvHJ\nx9M33UqDzB6k586G+DjICf2l7uDHxrJxxVJ6nnYmY8fcQofefUlKbMrXM/4FBHKlfY9jQi5fkVrX\n529mvwHOd84N9cYHASc750YEzTMMGAbQsWPHvuvX1/yql3Apuh69dEiUlp+bS8aeXTRv2w4IXCu8\nd8tmWiaHvsSy6MU6fMJZh6m2NZefm0tO5o8hf46gtvn774fTYP8AMhP/xe8eC3xxNvamD8FlM2Li\nJRGuXeUmD/4LOQ36ATMZPqH8O7XLU3TstDt1LZdeMxSA8Td+SGF0XMjjKT8vj6ioqDJvXM458nNz\njlhXSODeg8CJ1uTBD5PT4BRwsxj+3BO16jVxqHXZvWkjzdu1Iyqq7IlBONTJO3ydcxOBiRA4849w\ndSpUWegXiYmLKw5+gKio6AqDv7aKiYsjJi4u0tWosjGPFl1RdfAFahYHVlf2oejTUs1fBjnJs8nP\nyCK4LYKVdy06gJkd0T7w8rphOPIfGg+70r+lFUmRCv/NQHArJHtlIhJCdaM/Jv7gm9zoP1b9pwyk\nfovU1T7zge5m1tnM4oCBwMxKlhHxKSvxj99kNA+cF2Y1rfpvR0nlInLm75zLN7MRwEdANPC8c25Z\nJOoiUuu5QOpX98w/OubI9C+Hy21PFt3gFPk+/vokYn3+zrn3gfcjtX2ROiMCV0hJ/acneYnUcoVR\ngYfDFMQe2k9uF0mIrR03K0ntovAXqeW6DTieQnuHq+6+r/KZRaqo1l7qKRJOcdmfkh+TSV3oRz7r\nogGcddGAymcMIaaW/EyB1C4Kf/GlG1+s/ElZ9UV8FZ7zK/6jbh8RER9S+IvUcwkJdeVOZjmSFP4i\nIj6k8Bep52Ljqv8MBqm/FP4iIj6k8Bep5xo30Zm/lKXwFxHxIYW/SD2XkBD6aVfiXwp/EREfUviL\n1HNHJSVFugpSCyn8RUR8SOEvIuJDCn8RER9S+IuI+JDCX0TEhxT+IiI+pPAXEfEhhb+IiA8p/EVE\nfEjhLyLiQwp/kXoqOj8r0lWQWkzhL1JPHWj9AYXRb0e6GlJLxUS6AiISHmMeGRvpKkgtpjN/EREf\nUviLiPhQ2MLfzB4zs+/MbLGZvW1mzYKm3W1mq81spZmdF646iIhI+cJ55j8b6OWcOw5YBdwNYGY9\ngYHAscD5wDgziw5jPUREpJSwhb9z7mPnXL43Og9I9oYvBqY553Kccz8Aq4GTwlUPEREp60j1+V8P\nfOANtwc2Bk3b5JWVYGbDzCzNzNJ27tx5BKooIuIfNbrU08zmAG3LmXSvc26GN8+9QD7w6qGs2zk3\nEZgIkJqa6mpSTxERKalG4e+cO6ei6WZ2HXARcLZzrijANwMdgmZL9spEROQICefVPucDdwK/cs5l\nBk2aCQw0s3gz6wx0B74OVz1ERKSscN7hOxaIB2abGcA859zNzrllZvY6sJxAd9Bw51xBGOshIiKl\nhC38nXPdKpj2MPBwuLYtIiIV0x2+IiI+pPAXEfEhhb+IiA8p/EVEfEjhLyLiQwp/EREfUviLiPiQ\nwl9ExIcU/iIiPqTwFxHxIYW/iIgPKfxFRHxI4S8i4kMKfxERH1L4i4j4kMJfRMSHFP4iIj6k8BcR\n8SGFv4iIDyn8RUR8SOEvIuJDCn8RER9S+IuI+JDCX0TEhxT+IiI+pPAXEfEhhb+IiA8p/EVEfEjh\nLyLiQ2EPfzO7w8ycmbXyxs3Mnjaz1Wa22Mz6hLsOIlJ/xGVvi3QV6oWwhr+ZdQDOBTYEFV8AdPf+\nhgHjw1kHEalfEk/bRWHSh5GuRp0XE+b1PwHcCcwIKrsYeNk554B5ZtbMzJKcc1vDXBcRqQcG3vDb\nSFehXgjbmb+ZXQxsds4tKjWpPbAxaHyTV1Z6+WFmlmZmaTt37gxXNUVEfKlGZ/5mNgdoW86ke4F7\nCHT5VItzbiIwESA1NdVVdz0iIlJWjcLfOXdOeeVm9lOgM7DIzACSgW/M7CRgM9AhaPZkr0xERI6Q\nsHT7OOeWOOeOcs6lOOdSCHTt9HHObQNmAtd6V/30A/arv19E5MgK9xe+5XkfuBBYDWQCQyJQBxER\nXzsi4e+d/RcNO2D4kdiuiIiUT3f4ioj4kMJfRMSHFP4iIj6k8BcR8SGFv4iIDyn8RUR8SOEvIuJD\nCv86IDZ3X6SrICL1jMK/lstN+BeZnb6IdDVEpJ6JxM87yCG47clxka6CiNRDOvMXEfEhhb+IiA8p\n/EVEfEjhLyLiQwp/EREfUviLiPiQwl9ExIcU/iIiPqTwFxHxIYW/iIgPKfxFRHxI4S8i4kMKfxER\nH1L4i4j4kMJfRMSHFP4iIj6k8BcR8SGFv4iIDyn8RUR8KKzhb2Yjzew7M1tmZn8LKr/bzFab2Uoz\nOy+cdRARkbLC9gB3MzsTuBjo7ZzLMbOjvPKewEDgWKAdMMfMjnbOFYSrLiIiUlI4z/xvAf7qnMsB\ncM7t8MovBqY553Kccz8Aq4GTwlgPEREpJZzhfzRwmpl9ZWafmtmJXnl7YGPQfJu8shLMbJiZpZlZ\n2s6dO8NYTRER/6lRt4+ZzQHaljPpXm/dLYB+wInA62bWparrds5NBCYCpKamuprUU0RESqpR+Dvn\nzgk1zcxuAd5yzjngazMrBFoBm4EOQbMme2UiInKEhLPb5x3gTAAzOxqIA3YBM4GBZhZvZp2B7sDX\nYayHiIiUErarfYDngefNbCmQCwz2PgUsM7PXgeVAPjBcV/qIiBxZYQt/51wucE2IaQ8DD4dr2yIi\nUjHd4Ssi4kMKfxERH1L4i4j4kMJfRMSHFP4iIj6k8BcR8SGFv4iIDyn8RUR8SOEvIuJDCn8RER9S\n+IuI+JDCX0TEhxT+IiI+pPAXEfEhhb+IiA8p/EVEfEjhLyLiQwp/EREfUviLiPiQwl9ExIcU/iIi\nPqTwFxHxIYW/iIgPKfxFRHxI4S8i4kMKfxERH1L4i4j4kMJfRMSHFP4iIj4UtvA3s+PNbJ6ZLTSz\nNDM7ySs3M3vazFab2WIz6xOuOoiISPnCeeb/N+BB59zxwB+8cYALgO7e3zBgfBjrICIi5Qhn+Dug\niTfcFNjiDV8MvOwC5gHNzCwpjPUQEZFSYsK47tHAR2b2dwJvMj/zytsDG4Pm2+SVbQ1e2MyGEfhk\nQMeOHcNYTRER/6lR+JvZHKBtOZPuBc4GbnPOvWlmlwNTgHOqum7n3ERgIkBqaqqrST1FRKSkGoW/\ncy5kmJvZy8Aob/QNYLI3vBnoEDRrslcmIiJHSDj7/LcAp3vDZwHfe8MzgWu9q376Afudc1vLW4GI\niIRHOPv8bwSeMrMYIBuv/x54H7gQWA1kAkPCWAcRESlH2MLfOfdfoG855Q4YHq7tiohI5XSHr4iI\nDyn8RUR8SOEvIuJDCn8RER9S+IuI+JDCX0TEhxT+IiI+pPAXEfEhhb+IiA8p/EVEfEjhLyLiQwp/\nEREfUviLiPiQwl9ExIcU/iIiPqTwFxHxIYW/iIgPKfxFRHxI4S8i4kMKfxERH1L4i4j4kMJfRMSH\nFP4iIj6k8BcR8SGFv4iIDyn8RUR8SOEvIuJDCn8RER9S+IuI+FCNwt/MLjOzZWZWaGappabdbWar\nzWylmZ0XVH6+V7bazO6qyfZFRKR6anrmvxT4NfBZcKGZ9QQGAscC5wPjzCzazKKBZ4ELgJ7Ald68\nIiJ1jhW+S1bjNyJdjWqJqcnCzrkVAGZWetLFwDTnXA7wg5mtBk7ypq12zq31lpvmzbu8JvUQEYmE\nWyc+HukqVFu4+vzbAxuDxjd5ZaHKyzCzYWaWZmZpO3fuDFM1RUT8qdIzfzObA7QtZ9K9zrkZh79K\nAc65icBEgNTUVBeu7YiI+FGl4e+cO6ca690MdAgaT/bKqKBcRESOkHB1+8wEBppZvJl1BroDXwPz\nge5m1tnM4gh8KTwzTHUQEZEQavSFr5ldCjwDtAbeM7OFzrnznHPLzOx1Al/k5gPDnXMF3jIjgI+A\naOB559yyGu2BiIgcMnOu9nenp6amurS0tEhXQ0SkTjGzBc651PKm6Q5fEREfUviLiPhQnej2MbOd\nwPoarKIVsOswVac+UztVjdqpatROVRPOdurknGtd3oQ6Ef41ZWZpofq95CC1U9WonapG7VQ1kWon\ndfuIiPiQwl9ExIf8Ev4TI12BOkLtVDVqp6pRO1VNRNrJF33+IiJSkl/O/EVEJIjCX0TEh+p1+OuR\nkWBm68xsiZktNLM0r6yFmc02s++9f5t75WZmT3vttdjM+gStZ7A3//dmNjhS+3O4mNnzZrbDzJYG\nlR22djGzvl67r/aWLfPEo7oiRFs9YGabveNqoZldGDTtkB7h6v3Q41de+XTvRx/rFDPrYGZzzWy5\n92jbUV557T2mnHP18o/AD8etAboAccAioGek6xWBdlgHtCpV9jfgLm/4LuBRb/hC4APAgH7AV155\nC2Ct929zb7h5pPethu3yc6APsDQc7ULgV2z7ect8AFwQ6X0+zG31ADCmnHl7eq+1eKCz9xqMruj1\nCLwODPSGJwC3RHqfq9FGSUAfbzgRWOW1Ra09purzmf9JeI+MdM7lAkWPjJRAO7zkDb8EXBJU/rIL\nmAc0M7Mk4DxgtnNuj3NuLzCbwLOZ6yzn3GfAnlLFh6VdvGlNnHPzXOBV+3LQuuqcEG0VSvEjXJ1z\nPwBFj3At9/Xonb2eBfzLWz643esM59xW59w33nAGsILAUwpr7TFVn8O/yo+MrOcc8LGZLTCzYV5Z\nG+fcVm94G9DGG67x4zfruMPVLu294dLl9c0Ir8vi+aLuDA69rVoC+5xz+aXK6ywzSwFOAL6iFh9T\n9Tn8JeBU51wf4AJguJn9PHiidxah631LUbtUajzQFTge2Ar8I7LVqR3MrDHwJjDaOZcePK22HVP1\nOfwrepSkbzjnNnv/7gDeJvDxe7v3MRLv3x3e7KHazC9tebjaZbM3XLq83nDObXfOFTjnCoFJBI4r\nOPS22k2gyyOmVHmdY2axBIL/VefcW15xrT2m6nP4+/6RkWbWyMwSi4aBc4GlBNqh6CqCwcAMb3gm\ncK13JUI/YL/3kfUj4Fwza+59vD/XK6tvDku7eNPSzayf16d9bdC66oWiQPNcSuC4gkN8hKt3NjwX\n+I23fHC71xne//MUYIVz7vGgSbX3mIr0t+Th/CPwjfoqAlcZ3Bvp+kRg/7sQuKpiEbCsqA0I9LN+\nAnwPzAFaeOUGPOu11xIgNWhd1xP48m41MCTS+3YY2uY1At0VeQT6T284nO0CpBIIxDXAWLy76evi\nX4i2muq1xWICQZYUNP+93n6vJOiKlFCvR+84/dprwzeA+EjvczXa6FQCXTqLgYXe34W1+ZjSzzuI\niPhQfe72ERGREBT+IiI+pPAXEfEhhb+IiA8p/EVEfEjhLyLiQwp/EREf+n8qOkr3Wkf61gAAAABJ\nRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WU1PlXHoGKLW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "455c531a-51c7-4e42-ad58-19eed58a6920"
      },
      "source": [
        "from sklearn.datasets import load_wine\n",
        "import pandas as pd\n",
        "\n",
        "from keras.utils import to_categorical\n",
        "#x = data['data']\n",
        "#print(x.shape)\n",
        "\n",
        "#y = data['target']\n",
        "\n",
        "#print(a)\n",
        "\n",
        "#TODO: construct the output array manually by filling 3 arrays each with 50 0s 50 ones and 50 twos\n",
        "\n",
        "X, y = load_wine(return_X_y=True)\n",
        "#y = pd.DataFrame(y)\n",
        "print(X)\n",
        "print(y)\n",
        "from keras.utils import to_categorical\n",
        "y= to_categorical(y)\n",
        "print(X.shape)\n",
        "print(y.shape)\n",
        "#stack y \n",
        "#print(y)\n",
        "#print(y.shape)\n",
        "inputs = tf.keras.Input(shape=(13,))#Input(shape=(1,3))#Debt_data_change_of_change \n",
        "first = tf.keras.layers.Dense(10, activation='relu')(inputs)\n",
        "A = tf.keras.layers.Dense(400, activation='relu')(first)    \n",
        "x = tf.keras.layers.Dense(800, activation='relu')(A)#remeber the prediction probably needs to be shifted back two before it can actually be compared to actual\n",
        "x = tf.keras.layers.Dense(400,activation='relu')(x)\n",
        "x = tf.keras.layers.Dense(10,activation='relu')(x)\n",
        " #\n",
        "outputs = tf.keras.layers.Dense(3, activation='sigmoid')(x)\n",
        "model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
        "loss = tf.keras.losses.CategoricalCrossentropy()\n",
        "optimizer = tf.keras.optimizers.RMSprop\n",
        "#Use categorial crossentropy when doing stuff\n",
        "model.compile(optimizer='Adam',\n",
        "        loss=loss, metrics=['accuracy'])\n",
        "\t\n",
        "model.fit(X,y,epochs=10, verbose=1,validation_split=0.2)#, callbacks=[early_stop])\n",
        "\n",
        "Prediction = model.predict(X)\n",
        "#print(Prediction)\n",
        "\n",
        "print(Prediction.shape)\n",
        "u = 219\n",
        "model.summary()"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
            " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
            " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
            " ...\n",
            " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
            " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
            " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
            "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n",
            "(178, 13)\n",
            "(178, 3)\n",
            "Train on 142 samples, validate on 36 samples\n",
            "Epoch 1/10\n",
            "142/142 [==============================] - 1s 5ms/sample - loss: 6.3000 - acc: 0.4155 - val_loss: 6.1642 - val_acc: 0.0000e+00\n",
            "Epoch 2/10\n",
            "142/142 [==============================] - 0s 508us/sample - loss: 1.2249 - acc: 0.5000 - val_loss: 7.4778 - val_acc: 0.0000e+00\n",
            "Epoch 3/10\n",
            "142/142 [==============================] - 0s 431us/sample - loss: 1.1793 - acc: 0.5000 - val_loss: 5.3347 - val_acc: 0.0000e+00\n",
            "Epoch 4/10\n",
            "142/142 [==============================] - 0s 513us/sample - loss: 0.9600 - acc: 0.5000 - val_loss: 2.4256 - val_acc: 0.0000e+00\n",
            "Epoch 5/10\n",
            "142/142 [==============================] - 0s 510us/sample - loss: 0.9130 - acc: 0.5000 - val_loss: 1.8783 - val_acc: 0.0000e+00\n",
            "Epoch 6/10\n",
            "142/142 [==============================] - 0s 446us/sample - loss: 0.8854 - acc: 0.5000 - val_loss: 2.9503 - val_acc: 0.0000e+00\n",
            "Epoch 7/10\n",
            "142/142 [==============================] - 0s 475us/sample - loss: 0.8900 - acc: 0.5000 - val_loss: 3.3031 - val_acc: 0.0000e+00\n",
            "Epoch 8/10\n",
            "142/142 [==============================] - 0s 437us/sample - loss: 0.8953 - acc: 0.5000 - val_loss: 2.8390 - val_acc: 0.0000e+00\n",
            "Epoch 9/10\n",
            "142/142 [==============================] - 0s 448us/sample - loss: 0.8772 - acc: 0.5000 - val_loss: 2.2569 - val_acc: 0.0000e+00\n",
            "Epoch 10/10\n",
            "142/142 [==============================] - 0s 484us/sample - loss: 0.8680 - acc: 0.5000 - val_loss: 2.3593 - val_acc: 0.0000e+00\n",
            "(178, 3)\n",
            "Model: \"model_19\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_20 (InputLayer)        [(None, 13)]              0         \n",
            "_________________________________________________________________\n",
            "dense_114 (Dense)            (None, 10)                140       \n",
            "_________________________________________________________________\n",
            "dense_115 (Dense)            (None, 400)               4400      \n",
            "_________________________________________________________________\n",
            "dense_116 (Dense)            (None, 800)               320800    \n",
            "_________________________________________________________________\n",
            "dense_117 (Dense)            (None, 400)               320400    \n",
            "_________________________________________________________________\n",
            "dense_118 (Dense)            (None, 10)                4010      \n",
            "_________________________________________________________________\n",
            "dense_119 (Dense)            (None, 3)                 33        \n",
            "=================================================================\n",
            "Total params: 649,783\n",
            "Trainable params: 649,783\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}